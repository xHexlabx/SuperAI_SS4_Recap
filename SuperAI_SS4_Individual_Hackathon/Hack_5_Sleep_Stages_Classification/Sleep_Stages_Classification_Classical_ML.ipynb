{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperAI Season 4 - Level Individual Hackathon - Sleep Stages Classification <br> Classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./database/arrays.npy' , 'rb') as file :\n",
    "\n",
    "    arrays = np.load(file)\n",
    "\n",
    "with open ('./database/labels.npy' , 'rb') as file :\n",
    "\n",
    "    labels = np.load(file)\n",
    "    \n",
    "with open ('./database/test.npy' , 'rb') as file :\n",
    "    \n",
    "    tests = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66607, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0 : 'W' ,\n",
    "    1 : 'N' ,\n",
    "    2 : 'R'\n",
    "}\n",
    "label2id = {\n",
    "    'W' : 0 ,\n",
    "    'N' : 1 ,\n",
    "    'R' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BVP',\n",
       " 'ACC_X',\n",
       " 'ACC_Y',\n",
       " 'ACC_Z',\n",
       " 'TEMP',\n",
       " 'EDA',\n",
       " 'HR',\n",
       " 'IBI',\n",
       " 'second',\n",
       " 'minute',\n",
       " 'hour']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./datasets/train/train001.csv').drop(columns = 'Sleep_Stage')\n",
    "columns = list(train_df.columns)\n",
    "columns += ['second' , 'minute' , 'hour']\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = pd.DataFrame(arrays , columns = columns)\n",
    "test_df = pd.DataFrame(tests , columns = columns)\n",
    "labels_df = [ id2label[i] for i in list(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BVP</th>\n",
       "      <th>ACC_X</th>\n",
       "      <th>ACC_Y</th>\n",
       "      <th>ACC_Z</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>EDA</th>\n",
       "      <th>HR</th>\n",
       "      <th>IBI</th>\n",
       "      <th>second</th>\n",
       "      <th>minute</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.910635</td>\n",
       "      <td>-17.176043</td>\n",
       "      <td>-62.616665</td>\n",
       "      <td>-0.017708</td>\n",
       "      <td>32.096001</td>\n",
       "      <td>0.065086</td>\n",
       "      <td>76.942337</td>\n",
       "      <td>0.973763</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.742552</td>\n",
       "      <td>-19.217709</td>\n",
       "      <td>-62.263542</td>\n",
       "      <td>1.728125</td>\n",
       "      <td>32.084667</td>\n",
       "      <td>0.064648</td>\n",
       "      <td>83.393333</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>44.992188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.284318</td>\n",
       "      <td>-19.209375</td>\n",
       "      <td>-62.053123</td>\n",
       "      <td>5.612500</td>\n",
       "      <td>32.120998</td>\n",
       "      <td>0.064637</td>\n",
       "      <td>81.851334</td>\n",
       "      <td>0.851318</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001896</td>\n",
       "      <td>-18.813541</td>\n",
       "      <td>-62.075001</td>\n",
       "      <td>6.913542</td>\n",
       "      <td>32.209000</td>\n",
       "      <td>0.065075</td>\n",
       "      <td>82.737999</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>44.992188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.880448</td>\n",
       "      <td>-17.939583</td>\n",
       "      <td>-62.457291</td>\n",
       "      <td>5.042708</td>\n",
       "      <td>32.307667</td>\n",
       "      <td>0.065523</td>\n",
       "      <td>86.277000</td>\n",
       "      <td>0.790926</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66602</th>\n",
       "      <td>0.959505</td>\n",
       "      <td>-35.055210</td>\n",
       "      <td>-32.696877</td>\n",
       "      <td>44.809376</td>\n",
       "      <td>35.388668</td>\n",
       "      <td>0.218761</td>\n",
       "      <td>70.799004</td>\n",
       "      <td>0.884603</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66603</th>\n",
       "      <td>-1.191453</td>\n",
       "      <td>-35.187500</td>\n",
       "      <td>-32.608334</td>\n",
       "      <td>44.567707</td>\n",
       "      <td>35.393333</td>\n",
       "      <td>0.222786</td>\n",
       "      <td>68.563667</td>\n",
       "      <td>0.869035</td>\n",
       "      <td>44.992188</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66604</th>\n",
       "      <td>0.427625</td>\n",
       "      <td>-35.396873</td>\n",
       "      <td>-32.571877</td>\n",
       "      <td>44.428123</td>\n",
       "      <td>35.395332</td>\n",
       "      <td>0.215601</td>\n",
       "      <td>68.799332</td>\n",
       "      <td>0.871020</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66605</th>\n",
       "      <td>-1.782359</td>\n",
       "      <td>-35.833332</td>\n",
       "      <td>-32.691666</td>\n",
       "      <td>44.068748</td>\n",
       "      <td>35.388668</td>\n",
       "      <td>0.210401</td>\n",
       "      <td>68.755997</td>\n",
       "      <td>0.869588</td>\n",
       "      <td>44.992188</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66606</th>\n",
       "      <td>1.924458</td>\n",
       "      <td>-36.049999</td>\n",
       "      <td>-32.869793</td>\n",
       "      <td>43.926041</td>\n",
       "      <td>35.404667</td>\n",
       "      <td>0.206387</td>\n",
       "      <td>68.824669</td>\n",
       "      <td>0.905631</td>\n",
       "      <td>14.992188</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66607 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BVP      ACC_X      ACC_Y      ACC_Z       TEMP       EDA  \\\n",
       "0      2.910635 -17.176043 -62.616665  -0.017708  32.096001  0.065086   \n",
       "1     -2.742552 -19.217709 -62.263542   1.728125  32.084667  0.064648   \n",
       "2     -0.284318 -19.209375 -62.053123   5.612500  32.120998  0.064637   \n",
       "3     -0.001896 -18.813541 -62.075001   6.913542  32.209000  0.065075   \n",
       "4     -0.880448 -17.939583 -62.457291   5.042708  32.307667  0.065523   \n",
       "...         ...        ...        ...        ...        ...       ...   \n",
       "66602  0.959505 -35.055210 -32.696877  44.809376  35.388668  0.218761   \n",
       "66603 -1.191453 -35.187500 -32.608334  44.567707  35.393333  0.222786   \n",
       "66604  0.427625 -35.396873 -32.571877  44.428123  35.395332  0.215601   \n",
       "66605 -1.782359 -35.833332 -32.691666  44.068748  35.388668  0.210401   \n",
       "66606  1.924458 -36.049999 -32.869793  43.926041  35.404667  0.206387   \n",
       "\n",
       "              HR       IBI     second  minute  hour  \n",
       "0      76.942337  0.973763  14.992188     0.0   0.0  \n",
       "1      83.393333  0.953125  44.992188     0.0   0.0  \n",
       "2      81.851334  0.851318  14.992188     1.0   0.0  \n",
       "3      82.737999  0.812500  44.992188     1.0   0.0  \n",
       "4      86.277000  0.790926  14.992188     2.0   0.0  \n",
       "...          ...       ...        ...     ...   ...  \n",
       "66602  70.799004  0.884603  14.992188    43.0   6.0  \n",
       "66603  68.563667  0.869035  44.992188    43.0   6.0  \n",
       "66604  68.799332  0.871020  14.992188    44.0   6.0  \n",
       "66605  68.755997  0.869588  44.992188    44.0   6.0  \n",
       "66606  68.824669  0.905631  14.992188    45.0   6.0  \n",
       "\n",
       "[66607 rows x 11 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels    [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.Series({'labels' : labels_df})\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Length of label=1 and length of data=66607 is different.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\n\u001b[0;32m      2\u001b[0m     iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      4\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_df , labels_df)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:5220\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5218\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline, use_best_model,\n\u001b[0;32m   5221\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5222\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:2385\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES \u001b[38;5;241m+\u001b[39m (Pool,)):\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2385\u001b[0m train_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_train_params(\n\u001b[0;32m   2386\u001b[0m     X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, cat_features\u001b[38;5;241m=\u001b[39mcat_features, text_features\u001b[38;5;241m=\u001b[39mtext_features, embedding_features\u001b[38;5;241m=\u001b[39membedding_features,\n\u001b[0;32m   2387\u001b[0m     pairs\u001b[38;5;241m=\u001b[39mpairs, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, group_id\u001b[38;5;241m=\u001b[39mgroup_id, group_weight\u001b[38;5;241m=\u001b[39mgroup_weight,\n\u001b[0;32m   2388\u001b[0m     subgroup_id\u001b[38;5;241m=\u001b[39msubgroup_id, pairs_weight\u001b[38;5;241m=\u001b[39mpairs_weight, baseline\u001b[38;5;241m=\u001b[39mbaseline, use_best_model\u001b[38;5;241m=\u001b[39muse_best_model,\n\u001b[0;32m   2389\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39meval_set, verbose\u001b[38;5;241m=\u001b[39mverbose, logging_level\u001b[38;5;241m=\u001b[39mlogging_level, plot\u001b[38;5;241m=\u001b[39mplot, plot_file\u001b[38;5;241m=\u001b[39mplot_file,\n\u001b[0;32m   2390\u001b[0m     column_description\u001b[38;5;241m=\u001b[39mcolumn_description, verbose_eval\u001b[38;5;241m=\u001b[39mverbose_eval, metric_period\u001b[38;5;241m=\u001b[39mmetric_period,\n\u001b[0;32m   2391\u001b[0m     silent\u001b[38;5;241m=\u001b[39msilent, early_stopping_rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds, save_snapshot\u001b[38;5;241m=\u001b[39msave_snapshot,\n\u001b[0;32m   2392\u001b[0m     snapshot_file\u001b[38;5;241m=\u001b[39msnapshot_file, snapshot_interval\u001b[38;5;241m=\u001b[39msnapshot_interval, init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[0;32m   2393\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks\n\u001b[0;32m   2394\u001b[0m )\n\u001b[0;32m   2395\u001b[0m params \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2396\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:2265\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2262\u001b[0m text_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(text_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2263\u001b[0m embedding_features \u001b[38;5;241m=\u001b[39m _process_feature_indices(embedding_features, X, params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2265\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0;32m   2266\u001b[0m                                sample_weight, group_id, group_weight, subgroup_id, pairs_weight,\n\u001b[0;32m   2267\u001b[0m                                baseline, column_description)\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_pool\u001b[38;5;241m.\u001b[39mis_empty_:\n\u001b[0;32m   2269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:1503\u001b[0m, in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1502\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1503\u001b[0m     train_pool \u001b[38;5;241m=\u001b[39m Pool(X, y, cat_features\u001b[38;5;241m=\u001b[39mcat_features, text_features\u001b[38;5;241m=\u001b[39mtext_features, embedding_features\u001b[38;5;241m=\u001b[39membedding_features, pairs\u001b[38;5;241m=\u001b[39mpairs, weight\u001b[38;5;241m=\u001b[39msample_weight, group_id\u001b[38;5;241m=\u001b[39mgroup_id,\n\u001b[0;32m   1504\u001b[0m                       group_weight\u001b[38;5;241m=\u001b[39mgroup_weight, subgroup_id\u001b[38;5;241m=\u001b[39msubgroup_id, pairs_weight\u001b[38;5;241m=\u001b[39mpairs_weight, baseline\u001b[38;5;241m=\u001b[39mbaseline)\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_pool\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:848\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr, data_can_be_none)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    843\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    846\u001b[0m             )\n\u001b[1;32m--> 848\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m    849\u001b[0m                    group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_can_be_none:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:1429\u001b[0m, in \u001b[0;36mPool._init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(label)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1428\u001b[0m         label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(label, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_label_shape(label, samples_count)\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(feature_names, features_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\catboost\\core.py:969\u001b[0m, in \u001b[0;36mPool._check_label_shape\u001b[1;34m(self, label, samples_count)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03mCheck label length and dimension.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(label) \u001b[38;5;241m!=\u001b[39m samples_count:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of label=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and length of data=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is different.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(label), samples_count))\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Length of label=1 and length of data=66607 is different."
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=1,\n",
    "    depth=12,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "model.fit(train_df , labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./submissions/sample_submission.csv' , index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'R',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'R',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'N',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " 'W',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['labels'] = [i[0] for i in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
