{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperAI Season 4 - Level 2 Hackathon - Forest Type Double Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(row) :\n",
    "\n",
    "    row['NDVI'] = (row['b8'] - row['b4']) / (row['b8'] + row['b4'])\n",
    "    row['EVI'] = 2.5 * ((row['b8'] - row['b4']) / (row['b8'] + 6 * row['b4'] - 7.5 * row['b2'] + 1.01))\n",
    "    row['NDWI '] = (row['b3'] - row['b8']) / (row['b3'] + row['b8'])\n",
    "    row['SAVI '] = (row['b8'] - row['b4']) * (1 + 0.5) / (row['b8'] + row['b4'] + 0.5)\n",
    "    row['MSAVI'] = (2 * row['b8'] + 1 - ( (2 * row['b8'] + 1) ** 2 - 8 * (row['b8'] - row['b4'])) ** (1 / 2)) / 2\n",
    "    row['GNDVI '] = (row['b8'] - row['b3']) / (row['b8'] + row['b3'])\n",
    "    row['RENDVI '] = (row['b8'] - row['b5']) / (row['b8'] + row['b5'])\n",
    "    row['NDMI '] = (row['b8'] - row['b11']) / (row['b8'] + row['b11'])\n",
    "    row['GRVI'] = (row['b3'] - row['b4']) / (row['b3'] + row['b4'])\n",
    "    row['TVI'] = ( (row['b8'] - row['b4']) / (row['b8'] + row['b4'] + 0.5) ) ** (1 / 2)\n",
    "    row['MCARI'] = ((row['b5'] - row['b4']) - 0.2 * (row['b5'] - row['b3'])) / (row['b5'] / row['b4'])\n",
    "    row['BSI'] =  ((row['b11'] + row['b4']) - (row['b8'] + row['b2'])) / ((row['b11'] + row['b4']) + (row['b8'] + row['b2']))\n",
    "    row['NBR'] = (row['b8'] - row['b12']) / (row['b8'] + row['b12'])\n",
    "    row['MSI'] = row['b11'] / row['b8']\n",
    "    row['RVI'] = row['b8'] / row['b4']\n",
    "    row['GCI'] = (row['b8'] / row['b3']) - 1\n",
    "\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "smote = SMOTE(random_state = 42 , sampling_strategy= 'all')\n",
    "\n",
    "DEF_NONDEF_df = pd.read_csv('./datasets/DEF_NONDEF.csv' , index_col='id' )\n",
    "DEF_NONDEF_df , label_df  = smote.fit_resample(DEF_NONDEF_df.drop(columns=['nforest_type']) , DEF_NONDEF_df['nforest_type'])\n",
    "DEF_NONDEF_df = DEF_NONDEF_df.join(label_df)\n",
    "\n",
    "DEF_NONDEF_df.sort_index().to_csv('./datasets/fix_DEF_NONDEF.csv' , index_label='id')\n",
    "\n",
    "MDF_DDF_df = pd.read_csv('./datasets/MDF_DDF.csv' , index_col='id')\n",
    "MDF_DDF_df , label_df  = smote.fit_resample(MDF_DDF_df.drop(columns=['nforest_type']) , MDF_DDF_df['nforest_type'])\n",
    "MDF_DDF_df = MDF_DDF_df.join(label_df)\n",
    "\n",
    "MDF_DDF_df.sort_index().to_csv('./datasets/fix_MDF_DDF.csv' , index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b8_a</th>\n",
       "      <th>b9</th>\n",
       "      <th>nforest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>289</td>\n",
       "      <td>1488</td>\n",
       "      <td>771</td>\n",
       "      <td>295</td>\n",
       "      <td>418</td>\n",
       "      <td>300</td>\n",
       "      <td>666</td>\n",
       "      <td>1579</td>\n",
       "      <td>1896</td>\n",
       "      <td>1937</td>\n",
       "      <td>2143</td>\n",
       "      <td>2368</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>738</td>\n",
       "      <td>2281</td>\n",
       "      <td>1240</td>\n",
       "      <td>764</td>\n",
       "      <td>912</td>\n",
       "      <td>841</td>\n",
       "      <td>1254</td>\n",
       "      <td>2048</td>\n",
       "      <td>2376</td>\n",
       "      <td>2299</td>\n",
       "      <td>2792</td>\n",
       "      <td>2686</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>884</td>\n",
       "      <td>2256</td>\n",
       "      <td>1491</td>\n",
       "      <td>807</td>\n",
       "      <td>856</td>\n",
       "      <td>824</td>\n",
       "      <td>1127</td>\n",
       "      <td>1590</td>\n",
       "      <td>1834</td>\n",
       "      <td>1692</td>\n",
       "      <td>2021</td>\n",
       "      <td>2074</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>380</td>\n",
       "      <td>2232</td>\n",
       "      <td>1276</td>\n",
       "      <td>465</td>\n",
       "      <td>753</td>\n",
       "      <td>666</td>\n",
       "      <td>1286</td>\n",
       "      <td>2387</td>\n",
       "      <td>2723</td>\n",
       "      <td>2651</td>\n",
       "      <td>3045</td>\n",
       "      <td>3084</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278</td>\n",
       "      <td>1494</td>\n",
       "      <td>694</td>\n",
       "      <td>261</td>\n",
       "      <td>401</td>\n",
       "      <td>303</td>\n",
       "      <td>671</td>\n",
       "      <td>1852</td>\n",
       "      <td>2162</td>\n",
       "      <td>2284</td>\n",
       "      <td>2463</td>\n",
       "      <td>2305</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20931</th>\n",
       "      <td>460</td>\n",
       "      <td>1879</td>\n",
       "      <td>886</td>\n",
       "      <td>427</td>\n",
       "      <td>645</td>\n",
       "      <td>455</td>\n",
       "      <td>976</td>\n",
       "      <td>2413</td>\n",
       "      <td>2958</td>\n",
       "      <td>3095</td>\n",
       "      <td>3367</td>\n",
       "      <td>2823</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20932</th>\n",
       "      <td>200</td>\n",
       "      <td>1475</td>\n",
       "      <td>637</td>\n",
       "      <td>193</td>\n",
       "      <td>402</td>\n",
       "      <td>217</td>\n",
       "      <td>627</td>\n",
       "      <td>2191</td>\n",
       "      <td>2841</td>\n",
       "      <td>2945</td>\n",
       "      <td>2839</td>\n",
       "      <td>2884</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20933</th>\n",
       "      <td>130</td>\n",
       "      <td>1421</td>\n",
       "      <td>546</td>\n",
       "      <td>164</td>\n",
       "      <td>384</td>\n",
       "      <td>218</td>\n",
       "      <td>561</td>\n",
       "      <td>2073</td>\n",
       "      <td>2556</td>\n",
       "      <td>3380</td>\n",
       "      <td>3017</td>\n",
       "      <td>2875</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20934</th>\n",
       "      <td>348</td>\n",
       "      <td>1783</td>\n",
       "      <td>843</td>\n",
       "      <td>386</td>\n",
       "      <td>610</td>\n",
       "      <td>351</td>\n",
       "      <td>984</td>\n",
       "      <td>2597</td>\n",
       "      <td>3170</td>\n",
       "      <td>3251</td>\n",
       "      <td>3422</td>\n",
       "      <td>3264</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20935</th>\n",
       "      <td>183</td>\n",
       "      <td>1489</td>\n",
       "      <td>614</td>\n",
       "      <td>166</td>\n",
       "      <td>264</td>\n",
       "      <td>154</td>\n",
       "      <td>665</td>\n",
       "      <td>2103</td>\n",
       "      <td>2828</td>\n",
       "      <td>2015</td>\n",
       "      <td>3116</td>\n",
       "      <td>2884</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20936 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        b1   b11   b12   b2   b3   b4    b5    b6    b7    b8  b8_a    b9  \\\n",
       "0      289  1488   771  295  418  300   666  1579  1896  1937  2143  2368   \n",
       "1      738  2281  1240  764  912  841  1254  2048  2376  2299  2792  2686   \n",
       "2      884  2256  1491  807  856  824  1127  1590  1834  1692  2021  2074   \n",
       "3      380  2232  1276  465  753  666  1286  2387  2723  2651  3045  3084   \n",
       "4      278  1494   694  261  401  303   671  1852  2162  2284  2463  2305   \n",
       "...    ...   ...   ...  ...  ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "20931  460  1879   886  427  645  455   976  2413  2958  3095  3367  2823   \n",
       "20932  200  1475   637  193  402  217   627  2191  2841  2945  2839  2884   \n",
       "20933  130  1421   546  164  384  218   561  2073  2556  3380  3017  2875   \n",
       "20934  348  1783   843  386  610  351   984  2597  3170  3251  3422  3264   \n",
       "20935  183  1489   614  166  264  154   665  2103  2828  2015  3116  2884   \n",
       "\n",
       "      nforest_type  \n",
       "0          NON_DEF  \n",
       "1          NON_DEF  \n",
       "2          NON_DEF  \n",
       "3          NON_DEF  \n",
       "4          NON_DEF  \n",
       "...            ...  \n",
       "20931          DEF  \n",
       "20932          DEF  \n",
       "20933          DEF  \n",
       "20934          DEF  \n",
       "20935          DEF  \n",
       "\n",
       "[20936 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEF_NONDEF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b8_a</th>\n",
       "      <th>b9</th>\n",
       "      <th>nforest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293</td>\n",
       "      <td>1927</td>\n",
       "      <td>1038</td>\n",
       "      <td>278</td>\n",
       "      <td>475</td>\n",
       "      <td>453</td>\n",
       "      <td>987</td>\n",
       "      <td>1773</td>\n",
       "      <td>2184</td>\n",
       "      <td>1900</td>\n",
       "      <td>2343</td>\n",
       "      <td>3039</td>\n",
       "      <td>MDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197</td>\n",
       "      <td>1598</td>\n",
       "      <td>697</td>\n",
       "      <td>201</td>\n",
       "      <td>347</td>\n",
       "      <td>228</td>\n",
       "      <td>682</td>\n",
       "      <td>1982</td>\n",
       "      <td>2449</td>\n",
       "      <td>2254</td>\n",
       "      <td>2685</td>\n",
       "      <td>2690</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>929</td>\n",
       "      <td>1975</td>\n",
       "      <td>1031</td>\n",
       "      <td>982</td>\n",
       "      <td>1020</td>\n",
       "      <td>856</td>\n",
       "      <td>1220</td>\n",
       "      <td>2051</td>\n",
       "      <td>2421</td>\n",
       "      <td>2392</td>\n",
       "      <td>2671</td>\n",
       "      <td>2683</td>\n",
       "      <td>MDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>1560</td>\n",
       "      <td>689</td>\n",
       "      <td>189</td>\n",
       "      <td>408</td>\n",
       "      <td>175</td>\n",
       "      <td>609</td>\n",
       "      <td>2117</td>\n",
       "      <td>2907</td>\n",
       "      <td>3024</td>\n",
       "      <td>3005</td>\n",
       "      <td>2955</td>\n",
       "      <td>MDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241</td>\n",
       "      <td>1944</td>\n",
       "      <td>1131</td>\n",
       "      <td>362</td>\n",
       "      <td>538</td>\n",
       "      <td>487</td>\n",
       "      <td>918</td>\n",
       "      <td>1549</td>\n",
       "      <td>1844</td>\n",
       "      <td>1702</td>\n",
       "      <td>2077</td>\n",
       "      <td>2043</td>\n",
       "      <td>MDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11725</th>\n",
       "      <td>170</td>\n",
       "      <td>1840</td>\n",
       "      <td>928</td>\n",
       "      <td>198</td>\n",
       "      <td>456</td>\n",
       "      <td>311</td>\n",
       "      <td>773</td>\n",
       "      <td>2016</td>\n",
       "      <td>2439</td>\n",
       "      <td>2511</td>\n",
       "      <td>2739</td>\n",
       "      <td>2905</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11726</th>\n",
       "      <td>126</td>\n",
       "      <td>1963</td>\n",
       "      <td>881</td>\n",
       "      <td>181</td>\n",
       "      <td>400</td>\n",
       "      <td>187</td>\n",
       "      <td>703</td>\n",
       "      <td>2577</td>\n",
       "      <td>3414</td>\n",
       "      <td>3602</td>\n",
       "      <td>3768</td>\n",
       "      <td>3373</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11727</th>\n",
       "      <td>298</td>\n",
       "      <td>1302</td>\n",
       "      <td>1018</td>\n",
       "      <td>324</td>\n",
       "      <td>515</td>\n",
       "      <td>557</td>\n",
       "      <td>773</td>\n",
       "      <td>879</td>\n",
       "      <td>975</td>\n",
       "      <td>1171</td>\n",
       "      <td>1116</td>\n",
       "      <td>1151</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11728</th>\n",
       "      <td>589</td>\n",
       "      <td>1816</td>\n",
       "      <td>947</td>\n",
       "      <td>594</td>\n",
       "      <td>714</td>\n",
       "      <td>553</td>\n",
       "      <td>982</td>\n",
       "      <td>1820</td>\n",
       "      <td>2135</td>\n",
       "      <td>2190</td>\n",
       "      <td>2444</td>\n",
       "      <td>2519</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>228</td>\n",
       "      <td>1469</td>\n",
       "      <td>643</td>\n",
       "      <td>208</td>\n",
       "      <td>382</td>\n",
       "      <td>238</td>\n",
       "      <td>637</td>\n",
       "      <td>2061</td>\n",
       "      <td>2695</td>\n",
       "      <td>2777</td>\n",
       "      <td>2966</td>\n",
       "      <td>2897</td>\n",
       "      <td>DDF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11730 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        b1   b11   b12   b2    b3   b4    b5    b6    b7    b8  b8_a    b9  \\\n",
       "0      293  1927  1038  278   475  453   987  1773  2184  1900  2343  3039   \n",
       "1      197  1598   697  201   347  228   682  1982  2449  2254  2685  2690   \n",
       "2      929  1975  1031  982  1020  856  1220  2051  2421  2392  2671  2683   \n",
       "3      132  1560   689  189   408  175   609  2117  2907  3024  3005  2955   \n",
       "4      241  1944  1131  362   538  487   918  1549  1844  1702  2077  2043   \n",
       "...    ...   ...   ...  ...   ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "11725  170  1840   928  198   456  311   773  2016  2439  2511  2739  2905   \n",
       "11726  126  1963   881  181   400  187   703  2577  3414  3602  3768  3373   \n",
       "11727  298  1302  1018  324   515  557   773   879   975  1171  1116  1151   \n",
       "11728  589  1816   947  594   714  553   982  1820  2135  2190  2444  2519   \n",
       "11729  228  1469   643  208   382  238   637  2061  2695  2777  2966  2897   \n",
       "\n",
       "      nforest_type  \n",
       "0              MDF  \n",
       "1              DDF  \n",
       "2              MDF  \n",
       "3              MDF  \n",
       "4              MDF  \n",
       "...            ...  \n",
       "11725          DDF  \n",
       "11726          DDF  \n",
       "11727          DDF  \n",
       "11728          DDF  \n",
       "11729          DDF  \n",
       "\n",
       "[11730 rows x 13 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDF_DDF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEF_NONDEF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEF_NONDEF_Dataset (Dataset) :\n",
    "\n",
    "    def __init__ (self , annotation_path , transform = None , target_transform = None) :\n",
    "\n",
    "        self.annotation_path = annotation_path\n",
    "        self.annotation_file = pd.read_csv(annotation_path , index_col = 'id')\n",
    "        self.annotation_arrays = self.annotation_file.drop(columns=['nforest_type']).to_numpy().astype('float32') \n",
    "        self.annotation_labels = (self.annotation_file['nforest_type']  == 'DEF').to_numpy().astype('float32')\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        return len(self.annotation_file)\n",
    "\n",
    "    def __getitem__ (self , idx) :\n",
    "\n",
    "        data  = self.annotation_arrays[idx]\n",
    "        label = self.annotation_labels[idx]\n",
    "        \n",
    "        if self.transform :\n",
    "\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform :\n",
    "\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_NONDEF_dataset = DEF_NONDEF_Dataset('./datasets/fix_DEF_NONDEF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each split\n",
    "train_size = int(0.8 * len(DEF_NONDEF_dataset))\n",
    "test_size = len(DEF_NONDEF_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset , test_dataset = random_split(DEF_NONDEF_dataset, [train_size , test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([128, 12]) torch.Size([128])\n",
      "torch.Size([108, 12]) torch.Size([108])\n"
     ]
    }
   ],
   "source": [
    "for x , y in train_loader :\n",
    "    \n",
    "    print(x.shape , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self ,num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features, 256)  # Increased neurons\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256) \n",
    "        self.dropout1 = nn.Dropout(p = 0.25)     # Increased dropout\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size = 5 , stride=1, padding=2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)       # Increased neurons\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(p= 0.25)     # Increased dropout\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 256)       # Added another layer\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc4 = nn.Linear(256, 256)       # Added another layer\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc5 = nn.Linear(256, 256)       # Added another layer\n",
    "        self.batch_norm5 = nn.BatchNorm1d(256)\n",
    "        self.dropout5 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc6 = nn.Linear(256, 256)       # Added another layer\n",
    "        self.batch_norm6 = nn.BatchNorm1d(256)\n",
    "        self.dropout6 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.output = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.fc1(x))  # Changed to relu\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "\n",
    "        r = self.conv1(x.unsqueeze(1))\n",
    "        r = r.squeeze()\n",
    "        \n",
    "        x = F.relu(self.fc2(x))  # Changed to relu\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        x = x + r\n",
    "\n",
    "        r = self.conv1(x.unsqueeze(1))\n",
    "        r = r.squeeze()\n",
    "        \n",
    "        x = F.relu(self.fc3(x))  # Changed to relu\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        x = x + r\n",
    "        \n",
    "        x = F.relu(self.fc4(x))  # Changed to relu\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        r = self.conv1(x.unsqueeze(1))\n",
    "        r = r.squeeze()\n",
    "        \n",
    "        x = F.relu(self.fc5(x))  # Changed to relu\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        x = x + r\n",
    "        \n",
    "        r = self.conv1(x.unsqueeze(1))\n",
    "        r = r.squeeze()\n",
    "        \n",
    "        x = F.relu(self.fc6(x))  # Changed to relu\n",
    "        x = self.batch_norm6(x)\n",
    "        x = self.dropout6(x)\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        x = x + r\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEF_NONDEF_Classifier(L.LightningModule):\n",
    "    \n",
    "    def __init__(self , num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.Classifier = Classifier(num_features)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def forward (self , x) :\n",
    "        \n",
    "        return self.Classifier(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        \n",
    "        test_loss = F.binary_cross_entropy_with_logits(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('test_loss', test_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_NONDEF_Classifier_model = DEF_NONDEF_Classifier(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEF_NONDEF_Classifier(\n",
      "  (Classifier): Classifier(\n",
      "    (fc1): Linear(in_features=12, out_features=256, bias=True)\n",
      "    (batch_norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout1): Dropout(p=0.25, inplace=False)\n",
      "    (conv1): Conv1d(1, 1, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout2): Dropout(p=0.25, inplace=False)\n",
      "    (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout3): Dropout(p=0.25, inplace=False)\n",
      "    (fc4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (batch_norm4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout4): Dropout(p=0.25, inplace=False)\n",
      "    (fc5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (batch_norm5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout5): Dropout(p=0.25, inplace=False)\n",
      "    (fc6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (batch_norm6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout6): Dropout(p=0.25, inplace=False)\n",
      "    (output): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (train_accuracy): BinaryAccuracy()\n",
      "  (val_accuracy): BinaryAccuracy()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(DEF_NONDEF_Classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type           | Params\n",
      "--------------------------------------------------\n",
      "0 | Classifier     | Classifier     | 335 K \n",
      "1 | train_accuracy | BinaryAccuracy | 0     \n",
      "2 | val_accuracy   | BinaryAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "335 K     Trainable params\n",
      "0         Non-trainable params\n",
      "335 K     Total params\n",
      "1.342     Total estimated model params size (MB)\n",
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f3abe803be4b1494ca38f45f84aab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs = 100)\n",
    "trainer.fit(model = DEF_NONDEF_Classifier_model, train_dataloaders = train_loader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a74b70aec804d10ba238a4bb6e5036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8517192006111145     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5730642676353455     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8517192006111145    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5730642676353455    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.5730642676353455, 'test_acc_epoch': 0.8517192006111145}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(DEF_NONDEF_Classifier_model , dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDF DDF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDF_DDF_Dataset (Dataset) :\n",
    "\n",
    "    def __init__ (self , annotation_path , transform = None , target_transform = None) :\n",
    "\n",
    "        self.annotation_path = annotation_path\n",
    "        self.annotation_file = pd.read_csv(annotation_path , index_col = 'id')\n",
    "        self.annotation_arrays = self.annotation_file.drop(columns=['nforest_type']).to_numpy().astype('float32')\n",
    "        self.annotation_labels = (self.annotation_file['nforest_type']  == 'MDF').to_numpy().astype('float32')\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        return len(self.annotation_file)\n",
    "\n",
    "    def __getitem__ (self , idx) :\n",
    "\n",
    "        data  = self.annotation_arrays[idx]\n",
    "        label = self.annotation_labels[idx]\n",
    "        \n",
    "        if self.transform :\n",
    "\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform :\n",
    "\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return data , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_dataset = MDF_DDF_Dataset('./datasets/fix_MDF_DDF.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each split\n",
    "train_size = int(0.8 * len(MDF_DDF_dataset))\n",
    "test_size = len(MDF_DDF_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset , test_dataset = random_split(MDF_DDF_dataset, [train_size , test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class MDF_DDF_Classifier(L.LightningModule):\n",
    "    \n",
    "    def __init__(self , num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.Classifier = Classifier(num_features)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def forward(self , x) :\n",
    "    \n",
    "        return self.Classifier(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        test_loss = F.binary_cross_entropy_with_logits(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('test_loss', test_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_Classifier_model = MDF_DDF_Classifier(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type           | Params\n",
      "--------------------------------------------------\n",
      "0 | Classifier     | Classifier     | 335 K \n",
      "1 | train_accuracy | BinaryAccuracy | 0     \n",
      "2 | val_accuracy   | BinaryAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "335 K     Trainable params\n",
      "0         Non-trainable params\n",
      "335 K     Total params\n",
      "1.342     Total estimated model params size (MB)\n",
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeb1c7419b34a789308e18d03846b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model = MDF_DDF_Classifier_model, train_dataloaders = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567c471253ba44ca8f6f1310e8fab1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6794543862342834     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6371230483055115     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6794543862342834    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6371230483055115    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.6371230483055115, 'test_acc_epoch': 0.6794543862342834}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(MDF_DDF_Classifier_model , dataloaders = test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
