{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperAI Season 4 - Level 2 Hackathon - Forest Type Double Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(row) :\n",
    "\n",
    "    row['NDVI'] = (row['b8'] - row['b4']) / (row['b8'] + row['b4'])\n",
    "    row['EVI'] = 2.5 * ((row['b8'] - row['b4']) / (row['b8'] + 6 * row['b4'] - 7.5 * row['b2'] + 1.01))\n",
    "    row['NDWI '] = (row['b3'] - row['b8']) / (row['b3'] + row['b8'])\n",
    "    row['SAVI '] = (row['b8'] - row['b4']) * (1 + 0.5) / (row['b8'] + row['b4'] + 0.5)\n",
    "    row['MSAVI'] = (2 * row['b8'] + 1 - ( (2 * row['b8'] + 1) ** 2 - 8 * (row['b8'] - row['b4'])) ** (1 / 2)) / 2\n",
    "    row['GNDVI '] = (row['b8'] - row['b3']) / (row['b8'] + row['b3'])\n",
    "    row['RENDVI '] = (row['b8'] - row['b5']) / (row['b8'] + row['b5'])\n",
    "    row['NDMI '] = (row['b8'] - row['b11']) / (row['b8'] + row['b11'])\n",
    "    row['GRVI'] = (row['b3'] - row['b4']) / (row['b3'] + row['b4'])\n",
    "    row['TVI'] = ( (row['b8'] - row['b4']) / (row['b8'] + row['b4'] + 0.5) ) ** (1 / 2)\n",
    "    row['MCARI'] = ((row['b5'] - row['b4']) - 0.2 * (row['b5'] - row['b3'])) / (row['b5'] / row['b4'])\n",
    "    row['BSI'] =  ((row['b11'] + row['b4']) - (row['b8'] + row['b2'])) / ((row['b11'] + row['b4']) + (row['b8'] + row['b2']))\n",
    "    row['NBR'] = (row['b8'] - row['b12']) / (row['b8'] + row['b12'])\n",
    "    row['MSI'] = row['b11'] / row['b8']\n",
    "    row['RVI'] = row['b8'] / row['b4']\n",
    "    row['GCI'] = (row['b8'] / row['b3']) - 1\n",
    "    \n",
    "    row['NDVI_664nm'] = (row['b8_a']-row['b4'])/(row['b8_a']+row['b4'])\n",
    "    row['NDVI_559nm'] = (row['b8_a']-row['b3'])/(row['b8_a']+row['b3'])\n",
    "    row['NDVI_1640nm'] = (row['b8_a']-row['b11'])/(row['b8_a']+row['b11'])\n",
    "    row['NDVI_2200nm'] = (row['b8_a']-row['b12'])/(row['b8_a']+row['b12'])\n",
    "\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "import pandas as pd\n",
    "\n",
    "smote = SMOTE(random_state = 42 , sampling_strategy= 'all')\n",
    "\n",
    "DEF_NONDEF_df = pd.read_csv('./datasets/DEF_NONDEF.csv' , index_col='id' )\n",
    "DEF_NONDEF_df , label_df  = smote.fit_resample(DEF_NONDEF_df.drop(columns=['nforest_type']) , DEF_NONDEF_df['nforest_type'])\n",
    "DEF_NONDEF_df = DEF_NONDEF_df.join(label_df)\n",
    "DEF_NONDEF_df = DEF_NONDEF_df.apply(add_features , axis = 1)\n",
    "# DEF_NONDEF_df = add_features_focus(DEF_NONDEF_df)\n",
    "\n",
    "DEF_NONDEF_df.sort_index().to_csv('./datasets/fix_DEF_NONDEF.csv' , index_label='id')\n",
    "\n",
    "MDF_DDF_df = pd.read_csv('./datasets/MDF_DDF.csv' , index_col='id')\n",
    "MDF_DDF_df , label_df  = smote.fit_resample(MDF_DDF_df.drop(columns=['nforest_type']) , MDF_DDF_df['nforest_type'])\n",
    "MDF_DDF_df = MDF_DDF_df.join(label_df)\n",
    "MDF_DDF_df = MDF_DDF_df.apply(add_features , axis = 1)\n",
    "\n",
    "# MDF_DDF_df = add_features_focus(MDF_DDF_df)\n",
    "\n",
    "MDF_DDF_df.sort_index().to_csv('./datasets/fix_MDF_DDF.csv' , index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEF_NONDEF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "import lightning as L\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEF_NONDEF_Dataset (Dataset) :\n",
    "\n",
    "    def __init__ (self , annotation_path , transform = None , target_transform = None) :\n",
    "\n",
    "        self.annotation_path = annotation_path\n",
    "        self.annotation_file = pd.read_csv(annotation_path , index_col = 'id')\n",
    "        self.annotation_arrays = self.annotation_file.drop(columns=['nforest_type']).to_numpy().astype('float32') \n",
    "        self.annotation_labels = (self.annotation_file['nforest_type']  == 'DEF').to_numpy().astype('float32')\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        return len(self.annotation_file)\n",
    "\n",
    "    def __getitem__ (self , idx) :\n",
    "\n",
    "        data  = self.annotation_arrays[idx]\n",
    "        label = self.annotation_labels[idx]\n",
    "        \n",
    "        if self.transform :\n",
    "\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform :\n",
    "\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return data , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_NONDEF_dataset = DEF_NONDEF_Dataset('./datasets/fix_DEF_NONDEF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each split\n",
    "train_size = int(0.9 * len(DEF_NONDEF_dataset))\n",
    "test_size = len(DEF_NONDEF_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset , test_dataset = random_split(DEF_NONDEF_dataset, [train_size , test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self ,num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features, 128)  # Increased neurons\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128) \n",
    "        self.dropout1 = nn.Dropout(p = 0.25)     # Increased dropout\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 128)       # Increased neurons\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(p= 0.25)     # Increased dropout\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc5 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout5 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm6 = nn.BatchNorm1d(128)\n",
    "        self.dropout6 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc7 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm7 = nn.BatchNorm1d(128)\n",
    "        self.dropout7 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc8 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm8 = nn.BatchNorm1d(128)\n",
    "        self.dropout8 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc9 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm9 = nn.BatchNorm1d(128)\n",
    "        self.dropout9 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc10 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm10 = nn.BatchNorm1d(128)\n",
    "        self.dropout10 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))  # Changed to relu\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))  # Changed to relu\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))  # Changed to relu\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))  # Changed to relu\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))  # Changed to relu\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # x = F.relu(self.fc6(x))  # Changed to relu\n",
    "        # x = self.batch_norm6(x)\n",
    "        # x = self.dropout6(x)\n",
    "        \n",
    "        # x = F.relu(self.fc7(x))  # Changed to relu\n",
    "        # x = self.batch_norm7(x)\n",
    "        # x = self.dropout7(x)\n",
    "        \n",
    "        # x = F.relu(self.fc8(x))  # Changed to relu\n",
    "        # x = self.batch_norm8(x)\n",
    "        # x = self.dropout8(x)\n",
    "        \n",
    "        # x = F.relu(self.fc9(x))  # Changed to relu\n",
    "        # x = self.batch_norm9(x)\n",
    "        # x = self.dropout9(x)\n",
    "        \n",
    "        # x = F.relu(self.fc10(x))  # Changed to relu\n",
    "        # x = self.batch_norm10(x)\n",
    "        # x = self.dropout10(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEF_NONDEF_Classifier(L.LightningModule):\n",
    "    \n",
    "    def __init__(self , num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.Classifier = Classifier(num_features)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.Classifier(x)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.Classifier(x)\n",
    "        \n",
    "        test_loss = F.binary_cross_entropy(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('test_loss', test_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_NONDEF_Classifier_model = DEF_NONDEF_Classifier(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type           | Params\n",
      "--------------------------------------------------\n",
      "0 | Classifier     | Classifier     | 155 K \n",
      "1 | train_accuracy | BinaryAccuracy | 0     \n",
      "2 | val_accuracy   | BinaryAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "155 K     Trainable params\n",
      "0         Non-trainable params\n",
      "155 K     Total params\n",
      "0.622     Total estimated model params size (MB)\n",
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbaa068e3f14325bbe427cf2aa02c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs = 100)\n",
    "trainer.fit(model = DEF_NONDEF_Classifier_model, train_dataloaders = train_loader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798069321aa945fa9b5c7b6cd3c3d1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8610315322875977     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3799695670604706     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8610315322875977    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3799695670604706    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.3799695670604706, 'test_acc_epoch': 0.8610315322875977}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(DEF_NONDEF_Classifier_model , dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDF DDF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDF_DDF_Dataset (Dataset) :\n",
    "\n",
    "    def __init__ (self , annotation_path , transform = None , target_transform = None) :\n",
    "\n",
    "        self.annotation_path = annotation_path\n",
    "        self.annotation_file = pd.read_csv(annotation_path , index_col = 'id')\n",
    "        self.annotation_arrays = self.annotation_file.drop(columns=['nforest_type']).to_numpy().astype('float32')\n",
    "        self.annotation_labels = (self.annotation_file['nforest_type']  == 'MDF').to_numpy().astype('float32')\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self) :\n",
    "\n",
    "        return len(self.annotation_file)\n",
    "\n",
    "    def __getitem__ (self , idx) :\n",
    "\n",
    "        data  = self.annotation_arrays[idx]\n",
    "        label = self.annotation_labels[idx]\n",
    "        \n",
    "        if self.transform :\n",
    "\n",
    "            data = self.transform(data)\n",
    "\n",
    "        if self.target_transform :\n",
    "\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return data , label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_dataset = MDF_DDF_Dataset('./datasets/fix_MDF_DDF.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of each split\n",
    "train_size = int(0.9 * len(MDF_DDF_dataset))\n",
    "test_size = len(MDF_DDF_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset , test_dataset = random_split(MDF_DDF_dataset, [train_size , test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier2(nn.Module):\n",
    "    \n",
    "    def __init__(self ,num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features, 128)  # Increased neurons\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128) \n",
    "        self.dropout1 = nn.Dropout(p = 0.25)     # Increased dropout\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 128)       # Increased neurons\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(p= 0.25)     # Increased dropout\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc5 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm5 = nn.BatchNorm1d(128)\n",
    "        self.dropout5 = nn.Dropout(p= 0.25)\n",
    "        \n",
    "        self.fc6 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm6 = nn.BatchNorm1d(128)\n",
    "        self.dropout6 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc7 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm7 = nn.BatchNorm1d(128)\n",
    "        self.dropout7 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc8 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm8 = nn.BatchNorm1d(128)\n",
    "        self.dropout8 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc9 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm9 = nn.BatchNorm1d(128)\n",
    "        self.dropout9 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.fc10 = nn.Linear(128, 128)       # Added another layer\n",
    "        self.batch_norm10 = nn.BatchNorm1d(128)\n",
    "        self.dropout10 = nn.Dropout(p = 0.25)\n",
    "        \n",
    "        self.output = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))  # Changed to relu\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))  # Changed to relu\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))  # Changed to relu\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))  # Changed to relu\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))  # Changed to relu\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # x = F.relu(self.fc6(x))  # Changed to relu\n",
    "        # x = self.batch_norm6(x)\n",
    "        # x = self.dropout6(x)\n",
    "        \n",
    "        # x = F.relu(self.fc7(x))  # Changed to relu\n",
    "        # x = self.batch_norm7(x)\n",
    "        # x = self.dropout7(x)\n",
    "        \n",
    "        # x = F.relu(self.fc8(x))  # Changed to relu\n",
    "        # x = self.batch_norm8(x)\n",
    "        # x = self.dropout8(x)\n",
    "        \n",
    "        # x = F.relu(self.fc9(x))  # Changed to relu\n",
    "        # x = self.batch_norm9(x)\n",
    "        # x = self.dropout9(x)\n",
    "        \n",
    "        # x = F.relu(self.fc10(x))  # Changed to relu\n",
    "        # x = self.batch_norm10(x)\n",
    "        # x = self.dropout10(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDF_DDF_Classifier(L.LightningModule):\n",
    "    \n",
    "    def __init__(self , num_features):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.Classifier = Classifier2(num_features)\n",
    "        \n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.Classifier(x)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = self.Classifier(x)\n",
    "        \n",
    "        test_loss = F.binary_cross_entropy(y_hat.squeeze() , y)\n",
    "        acc = self.train_accuracy(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('test_loss', test_loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_Classifier_model = MDF_DDF_Classifier(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type           | Params\n",
      "--------------------------------------------------\n",
      "0 | Classifier     | Classifier2    | 155 K \n",
      "1 | train_accuracy | BinaryAccuracy | 0     \n",
      "2 | val_accuracy   | BinaryAccuracy | 0     \n",
      "--------------------------------------------------\n",
      "155 K     Trainable params\n",
      "0         Non-trainable params\n",
      "155 K     Total params\n",
      "0.622     Total estimated model params size (MB)\n",
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6924575a325a4f05ad284f88a98535a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model = MDF_DDF_Classifier_model, train_dataloaders = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teehe\\appdata\\roaming\\python\\python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b33f573587444e78fe24fc7920f2600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7152600288391113     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5531030297279358     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7152600288391113    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5531030297279358    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.5531030297279358, 'test_acc_epoch': 0.7152600288391113}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(MDF_DDF_Classifier_model , dataloaders = test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
