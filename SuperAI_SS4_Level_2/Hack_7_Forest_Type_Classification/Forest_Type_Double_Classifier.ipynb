{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperAI Season 4 - Level 2 Hackathon - Forest Type Double Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./datasets/train.csv' , index_col='id')\n",
    "test_df = pd.read_csv('./datasets/test.csv' , index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NON-DEF (MDF / DDF) / DEF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_df = train_df[train_df['nforest_type'] == 'DEF']\n",
    "NON_DEF_df = train_df[train_df['nforest_type'] != 'DEF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_DEF_df.to_csv('./datasets/MDF_DDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SettingWithCopyError",
     "evalue": "\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSettingWithCopyError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24200\\1676613163.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNON_DEF_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nforest_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NON_DEF'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mDEF_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nforest_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DEF'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m         \u001b[0miloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0miloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_missing_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2238\u001b[0m                     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2239\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2240\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2241\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2242\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, clear, verify_is_copy, inplace)\u001b[0m\n\u001b[0;32m   3947\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3950\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverify_is_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3951\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_setitem_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"referent\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3953\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclear\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3954\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, t, force)\u001b[0m\n\u001b[0;32m   4404\u001b[0m                 \u001b[1;34m\"indexing.html#returning-a-view-versus-a-copy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4405\u001b[0m             )\n\u001b[0;32m   4406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4408\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mSettingWithCopyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"warn\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4410\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSettingWithCopyWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSettingWithCopyError\u001b[0m: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy"
     ]
    }
   ],
   "source": [
    "NON_DEF_df['nforest_type'] = 'NON_DEF'\n",
    "DEF_df['nforest_type'] = 'DEF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b8_a</th>\n",
       "      <th>b9</th>\n",
       "      <th>nforest_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>293</td>\n",
       "      <td>1927</td>\n",
       "      <td>1038</td>\n",
       "      <td>278</td>\n",
       "      <td>475</td>\n",
       "      <td>453</td>\n",
       "      <td>987</td>\n",
       "      <td>1773</td>\n",
       "      <td>2184</td>\n",
       "      <td>1900</td>\n",
       "      <td>2343</td>\n",
       "      <td>3039</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>197</td>\n",
       "      <td>1598</td>\n",
       "      <td>697</td>\n",
       "      <td>201</td>\n",
       "      <td>347</td>\n",
       "      <td>228</td>\n",
       "      <td>682</td>\n",
       "      <td>1982</td>\n",
       "      <td>2449</td>\n",
       "      <td>2254</td>\n",
       "      <td>2685</td>\n",
       "      <td>2690</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13312</th>\n",
       "      <td>929</td>\n",
       "      <td>1975</td>\n",
       "      <td>1031</td>\n",
       "      <td>982</td>\n",
       "      <td>1020</td>\n",
       "      <td>856</td>\n",
       "      <td>1220</td>\n",
       "      <td>2051</td>\n",
       "      <td>2421</td>\n",
       "      <td>2392</td>\n",
       "      <td>2671</td>\n",
       "      <td>2683</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>132</td>\n",
       "      <td>1560</td>\n",
       "      <td>689</td>\n",
       "      <td>189</td>\n",
       "      <td>408</td>\n",
       "      <td>175</td>\n",
       "      <td>609</td>\n",
       "      <td>2117</td>\n",
       "      <td>2907</td>\n",
       "      <td>3024</td>\n",
       "      <td>3005</td>\n",
       "      <td>2955</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>241</td>\n",
       "      <td>1944</td>\n",
       "      <td>1131</td>\n",
       "      <td>362</td>\n",
       "      <td>538</td>\n",
       "      <td>487</td>\n",
       "      <td>918</td>\n",
       "      <td>1549</td>\n",
       "      <td>1844</td>\n",
       "      <td>1702</td>\n",
       "      <td>2077</td>\n",
       "      <td>2043</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13977</th>\n",
       "      <td>1983</td>\n",
       "      <td>3602</td>\n",
       "      <td>2720</td>\n",
       "      <td>1622</td>\n",
       "      <td>1782</td>\n",
       "      <td>1766</td>\n",
       "      <td>2314</td>\n",
       "      <td>3488</td>\n",
       "      <td>3900</td>\n",
       "      <td>3924</td>\n",
       "      <td>4097</td>\n",
       "      <td>6053</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>940</td>\n",
       "      <td>2007</td>\n",
       "      <td>1148</td>\n",
       "      <td>975</td>\n",
       "      <td>1080</td>\n",
       "      <td>968</td>\n",
       "      <td>1252</td>\n",
       "      <td>1780</td>\n",
       "      <td>1983</td>\n",
       "      <td>1942</td>\n",
       "      <td>2247</td>\n",
       "      <td>2170</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1174</td>\n",
       "      <td>2312</td>\n",
       "      <td>1190</td>\n",
       "      <td>1112</td>\n",
       "      <td>1126</td>\n",
       "      <td>889</td>\n",
       "      <td>1310</td>\n",
       "      <td>2511</td>\n",
       "      <td>3085</td>\n",
       "      <td>3050</td>\n",
       "      <td>3396</td>\n",
       "      <td>3380</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15634</th>\n",
       "      <td>193</td>\n",
       "      <td>2091</td>\n",
       "      <td>1084</td>\n",
       "      <td>274</td>\n",
       "      <td>502</td>\n",
       "      <td>452</td>\n",
       "      <td>881</td>\n",
       "      <td>1953</td>\n",
       "      <td>2427</td>\n",
       "      <td>2830</td>\n",
       "      <td>2863</td>\n",
       "      <td>2586</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nforest_type</th>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "      <td>NON_DEF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   b1      b11      b12       b2       b3       b4       b5  \\\n",
       "id                                                                            \n",
       "2002              293     1927     1038      278      475      453      987   \n",
       "3212              197     1598      697      201      347      228      682   \n",
       "13312             929     1975     1031      982     1020      856     1220   \n",
       "17020             132     1560      689      189      408      175      609   \n",
       "5967              241     1944     1131      362      538      487      918   \n",
       "...               ...      ...      ...      ...      ...      ...      ...   \n",
       "13977            1983     3602     2720     1622     1782     1766     2314   \n",
       "755               940     2007     1148      975     1080      968     1252   \n",
       "1616             1174     2312     1190     1112     1126      889     1310   \n",
       "15634             193     2091     1084      274      502      452      881   \n",
       "nforest_type  NON_DEF  NON_DEF  NON_DEF  NON_DEF  NON_DEF  NON_DEF  NON_DEF   \n",
       "\n",
       "                   b6       b7       b8     b8_a       b9 nforest_type  \n",
       "id                                                                      \n",
       "2002             1773     2184     1900     2343     3039      NON_DEF  \n",
       "3212             1982     2449     2254     2685     2690      NON_DEF  \n",
       "13312            2051     2421     2392     2671     2683      NON_DEF  \n",
       "17020            2117     2907     3024     3005     2955      NON_DEF  \n",
       "5967             1549     1844     1702     2077     2043      NON_DEF  \n",
       "...               ...      ...      ...      ...      ...          ...  \n",
       "13977            3488     3900     3924     4097     6053      NON_DEF  \n",
       "755              1780     1983     1942     2247     2170      NON_DEF  \n",
       "1616             2511     3085     3050     3396     3380      NON_DEF  \n",
       "15634            1953     2427     2830     2863     2586      NON_DEF  \n",
       "nforest_type  NON_DEF  NON_DEF  NON_DEF  NON_DEF  NON_DEF      NON_DEF  \n",
       "\n",
       "[10469 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NON_DEF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b11</th>\n",
       "      <th>b12</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b8_a</th>\n",
       "      <th>b9</th>\n",
       "      <th>nforest_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14932</th>\n",
       "      <td>701</td>\n",
       "      <td>1776</td>\n",
       "      <td>895</td>\n",
       "      <td>689</td>\n",
       "      <td>905</td>\n",
       "      <td>684</td>\n",
       "      <td>1187</td>\n",
       "      <td>2504</td>\n",
       "      <td>2836</td>\n",
       "      <td>2839</td>\n",
       "      <td>3172</td>\n",
       "      <td>3103</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14572</th>\n",
       "      <td>156</td>\n",
       "      <td>1121</td>\n",
       "      <td>547</td>\n",
       "      <td>264</td>\n",
       "      <td>472</td>\n",
       "      <td>362</td>\n",
       "      <td>699</td>\n",
       "      <td>1673</td>\n",
       "      <td>1967</td>\n",
       "      <td>2182</td>\n",
       "      <td>2260</td>\n",
       "      <td>2277</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>536</td>\n",
       "      <td>1591</td>\n",
       "      <td>777</td>\n",
       "      <td>686</td>\n",
       "      <td>760</td>\n",
       "      <td>624</td>\n",
       "      <td>1049</td>\n",
       "      <td>2152</td>\n",
       "      <td>2600</td>\n",
       "      <td>2665</td>\n",
       "      <td>2964</td>\n",
       "      <td>2777</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10489</th>\n",
       "      <td>1551</td>\n",
       "      <td>1519</td>\n",
       "      <td>718</td>\n",
       "      <td>1730</td>\n",
       "      <td>1764</td>\n",
       "      <td>1514</td>\n",
       "      <td>1719</td>\n",
       "      <td>2267</td>\n",
       "      <td>2554</td>\n",
       "      <td>2598</td>\n",
       "      <td>2743</td>\n",
       "      <td>2743</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>313</td>\n",
       "      <td>1696</td>\n",
       "      <td>864</td>\n",
       "      <td>366</td>\n",
       "      <td>568</td>\n",
       "      <td>411</td>\n",
       "      <td>849</td>\n",
       "      <td>1965</td>\n",
       "      <td>2387</td>\n",
       "      <td>2439</td>\n",
       "      <td>2827</td>\n",
       "      <td>2759</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>182</td>\n",
       "      <td>1336</td>\n",
       "      <td>574</td>\n",
       "      <td>188</td>\n",
       "      <td>296</td>\n",
       "      <td>145</td>\n",
       "      <td>571</td>\n",
       "      <td>2000</td>\n",
       "      <td>2473</td>\n",
       "      <td>2243</td>\n",
       "      <td>2884</td>\n",
       "      <td>2844</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>1</td>\n",
       "      <td>1753</td>\n",
       "      <td>820</td>\n",
       "      <td>106</td>\n",
       "      <td>431</td>\n",
       "      <td>301</td>\n",
       "      <td>840</td>\n",
       "      <td>2091</td>\n",
       "      <td>2472</td>\n",
       "      <td>2762</td>\n",
       "      <td>2914</td>\n",
       "      <td>2440</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>281</td>\n",
       "      <td>1837</td>\n",
       "      <td>835</td>\n",
       "      <td>342</td>\n",
       "      <td>718</td>\n",
       "      <td>285</td>\n",
       "      <td>975</td>\n",
       "      <td>2801</td>\n",
       "      <td>3556</td>\n",
       "      <td>3637</td>\n",
       "      <td>3801</td>\n",
       "      <td>3148</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>1656</td>\n",
       "      <td>1955</td>\n",
       "      <td>965</td>\n",
       "      <td>1792</td>\n",
       "      <td>1972</td>\n",
       "      <td>1786</td>\n",
       "      <td>2013</td>\n",
       "      <td>2426</td>\n",
       "      <td>2637</td>\n",
       "      <td>2570</td>\n",
       "      <td>2839</td>\n",
       "      <td>2825</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16941</th>\n",
       "      <td>594</td>\n",
       "      <td>1480</td>\n",
       "      <td>692</td>\n",
       "      <td>620</td>\n",
       "      <td>694</td>\n",
       "      <td>566</td>\n",
       "      <td>847</td>\n",
       "      <td>1833</td>\n",
       "      <td>2489</td>\n",
       "      <td>2779</td>\n",
       "      <td>2684</td>\n",
       "      <td>2799</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2585 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         b1   b11  b12    b2    b3    b4    b5    b6    b7    b8  b8_a    b9  \\\n",
       "id                                                                             \n",
       "14932   701  1776  895   689   905   684  1187  2504  2836  2839  3172  3103   \n",
       "14572   156  1121  547   264   472   362   699  1673  1967  2182  2260  2277   \n",
       "7504    536  1591  777   686   760   624  1049  2152  2600  2665  2964  2777   \n",
       "10489  1551  1519  718  1730  1764  1514  1719  2267  2554  2598  2743  2743   \n",
       "7995    313  1696  864   366   568   411   849  1965  2387  2439  2827  2759   \n",
       "...     ...   ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "3762    182  1336  574   188   296   145   571  2000  2473  2243  2884  2844   \n",
       "7804      1  1753  820   106   431   301   840  2091  2472  2762  2914  2440   \n",
       "5566    281  1837  835   342   718   285   975  2801  3556  3637  3801  3148   \n",
       "9311   1656  1955  965  1792  1972  1786  2013  2426  2637  2570  2839  2825   \n",
       "16941   594  1480  692   620   694   566   847  1833  2489  2779  2684  2799   \n",
       "\n",
       "      nforest_type  \n",
       "id                  \n",
       "14932          DEF  \n",
       "14572          DEF  \n",
       "7504           DEF  \n",
       "10489          DEF  \n",
       "7995           DEF  \n",
       "...            ...  \n",
       "3762           DEF  \n",
       "7804           DEF  \n",
       "5566           DEF  \n",
       "9311           DEF  \n",
       "16941          DEF  \n",
       "\n",
       "[2585 rows x 13 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEF_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m DEF_NONDEF_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([NON_DEF_df , DEF_df])\n\u001b[1;32m----> 2\u001b[0m DEF_NONDEF_df\u001b[38;5;241m.\u001b[39msort_index()\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/DEF_NONDEF.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:7134\u001b[0m, in \u001b[0;36mDataFrame.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   7037\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort_index\u001b[39m(\n\u001b[0;32m   7038\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   7039\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   7048\u001b[0m     key: IndexKeyFunc \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   7049\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   7050\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   7051\u001b[0m \u001b[38;5;124;03m    Sort object by labels (along an axis).\u001b[39;00m\n\u001b[0;32m   7052\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   7132\u001b[0m \u001b[38;5;124;03m    d  4\u001b[39;00m\n\u001b[0;32m   7133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 7134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msort_index(\n\u001b[0;32m   7135\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   7136\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   7137\u001b[0m         ascending\u001b[38;5;241m=\u001b[39mascending,\n\u001b[0;32m   7138\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   7139\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[0;32m   7140\u001b[0m         na_position\u001b[38;5;241m=\u001b[39mna_position,\n\u001b[0;32m   7141\u001b[0m         sort_remaining\u001b[38;5;241m=\u001b[39msort_remaining,\n\u001b[0;32m   7142\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[0;32m   7143\u001b[0m         key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[0;32m   7144\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:5232\u001b[0m, in \u001b[0;36mNDFrame.sort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   5228\u001b[0m ascending \u001b[38;5;241m=\u001b[39m validate_ascending(ascending)\n\u001b[0;32m   5230\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 5232\u001b[0m indexer \u001b[38;5;241m=\u001b[39m get_indexer_indexer(\n\u001b[0;32m   5233\u001b[0m     target, level, ascending, kind, na_position, sort_remaining, key\n\u001b[0;32m   5234\u001b[0m )\n\u001b[0;32m   5236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\sorting.py:113\u001b[0m, in \u001b[0;36mget_indexer_indexer\u001b[1;34m(target, level, ascending, kind, na_position, sort_remaining, key)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# ascending can only be a Sequence for MultiIndex\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m nargsort(\n\u001b[0;32m    114\u001b[0m         target,\n\u001b[0;32m    115\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[0;32m    116\u001b[0m         ascending\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;28mbool\u001b[39m, ascending),\n\u001b[0;32m    117\u001b[0m         na_position\u001b[38;5;241m=\u001b[39mna_position,\n\u001b[0;32m    118\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\pandas\\core\\sorting.py:483\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    481\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    482\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 483\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[non_nans\u001b[38;5;241m.\u001b[39margsort(kind\u001b[38;5;241m=\u001b[39mkind)]\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    485\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "DEF_NONDEF_df = pd.concat([NON_DEF_df , DEF_df])\n",
    "DEF_NONDEF_df.sort_index().to_csv('./datasets/DEF_NONDEF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEF_NONDEF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_NONDEF_df = pd.read_csv('./datasets/DEF_NONDEF.csv' , index_col='id')\n",
    "test_df = pd.read_csv('./datasets/test.csv', index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_for_DEF_NONDEF(row) :\n",
    "\n",
    "    row['NDVI'] = (row['b8'] - row['b4']) / (row['b8'] + row['b4'])\n",
    "    row['EVI'] = 2.5 * ((row['b8'] - row['b4']) / (row['b8'] + 6 * row['b4'] - 7.5 * row['b2'] + 1.01))\n",
    "    row['NDWI '] = (row['b3'] - row['b8']) / (row['b3'] + row['b8'])\n",
    "    row['SAVI '] = (row['b8'] - row['b4']) * (1 + 0.5) / (row['b8'] + row['b4'] + 0.5)\n",
    "    row['MSAVI'] = (2 * row['b8'] + 1 - ( (2 * row['b8'] + 1) ** 2 - 8 * (row['b8'] - row['b4'])) ** (1 / 2)) / 2\n",
    "    row['GNDVI '] = (row['b8'] - row['b3']) / (row['b8'] + row['b3'])\n",
    "    row['RENDVI '] = (row['b8'] - row['b5']) / (row['b8'] + row['b5'])\n",
    "    row['NDMI '] = (row['b8'] - row['b11']) / (row['b8'] + row['b11'])\n",
    "    row['GRVI'] = (row['b3'] - row['b4']) / (row['b3'] + row['b4'])\n",
    "    row['TVI'] = ( (row['b8'] - row['b4']) / (row['b8'] + row['b4'] + 0.5) ) ** (1 / 2)\n",
    "    row['MCARI'] = ((row['b5'] - row['b4']) - 0.2 * (row['b5'] - row['b3'])) / (row['b5'] / row['b4'])\n",
    "    row['BSI'] =  ((row['b11'] + row['b4']) - (row['b8'] + row['b2'])) / ((row['b11'] + row['b4']) + (row['b8'] + row['b2']))\n",
    "    row['NBR'] = (row['b8'] - row['b12']) / (row['b8'] + row['b12'])\n",
    "    row['MSI'] = row['b11'] / row['b8']\n",
    "\n",
    "    return row\n",
    "\n",
    "def drop_features_for_DEF_NONDEF(df) :\n",
    "\n",
    "    return df\n",
    "    # return df.drop(columns = ['b1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:114: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 800/550'] = (b8 - b3) / (b8 + b3)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 800/650'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 800/675'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 800/680'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 819/1600'] = (b8 - b11) / (b8 + b11)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 819/1649'] = (b8 - b11) / (b8 + b11)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 820/1600'] = (b8 - b11) / (b8 + b11)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 827/668'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:122: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 833/1649'] = (b8 - b11) / (b8 + b11)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 833/658'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 860/1640'] = (b8a - b11) / (b8a + b11)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference 895/675'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference Green/Red'] = (b3 - b4) / (b3 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/Blue'] = (b8 - b2) / (b8 + b2)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/Green'] = (b8 - b3) / (b8 + b3)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:131: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/Red'] = (b8 - b4) / (b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/Rededge'] = (b8 - b5) / (b8 + b5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference Red/Green'] = (b4 - b3) / (b4 + b3)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference Salinity Index'] = (b11 - b12) / (b11 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:136: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference Vegetation Index 690-710'] = (b8 - b5) / (b8 + b5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Optimized Soil Adjusted Vegetation Index'] = (1 + 0.16) * (b8 - b4) / (b8 + b4 + 0.16)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:139: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Pan NDVI'] = (b8 - (b3 + b4 + b2)) / (b8 + (b3 + b4 + b2))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['RDVI'] = (b8 - b4) / ((b8 + b4).pow(0.5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:148: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['RDVI2'] = (b8 - b4) / ((b8 + b4).pow(0.5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red edge 1'] = b5 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red edge 2'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Blue NDVI'] = (b8 - (b4 + b2)) / (b8 + (b4 + b2))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 1'] = 700 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 2'] = 702 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 3'] = 705 + 35 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Position Linear Interpolation'] = 700 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Reflectance at the inflexion point'] = (b4 + b7) / 2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Renormalized Difference Vegetation Index'] = (b8 - b4) / ((b8 + b4).pow(0.5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:160: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Shape Index'] = (2 * b4 - b3 - b2) / (b3 - b2)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 1600/820'] = b11 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 1650/2218'] = b11 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 440/740'] = b1 / b6\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 450/550'] = b1 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:165: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 520/670'] = b2 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 550/800'] = b3 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 560/658'] = b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 675/555'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 675/705'] = b4 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 700'] = 1 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:171: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 710/670'] = b5 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 735/710'] = b6 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 774/677'] = b7 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 800/2170'] = b8 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 800/500'] = b8 / b2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 810/560'] = b8 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 833/1649'] = b8 / b11\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 833/658'] = b8 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 850/710'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 860/550'] = b8a / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 860/708'] = b8a / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/700-715'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/G'] = b8 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/RED'] = b8 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/Rededge'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/Blue'] = b4 / b2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/Green'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/NIR'] = b4 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Adjusted Vegetation Index'] = (b8 - b4) / (b8 + b4 + 0.48) * (1 + 0.48)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['SQRT(IR/R)'] = (b8 / b4).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:203: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Structure Intensive Pigment Index 1'] = (b8 - b1) / (b8 - b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Structure Intensive Pigment Index 3'] = (b8 - b2) / (b8 - b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TCARI/OSAVI'] = (3 * ((b5 - b4) - (0.2 * (b5 - b3) * (b5 / b4)))) / ((1 + 0.16) * ((b8 - b4)/(b8 + b4 + 0.16)))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Transformed Chlorophyll Absorbtion Ratio'] = 3 * ((b5 - b4) - (0.2 * (b5 - b3) * (b5 / b4)))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Transformed NDVI'] = ((b8 - b4) / (b8 + b4) + 0.5).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Triangular chlorophyll index'] = 1.2 * (b5 - b3) - 1.5 * (b4 - b3) * (b5 / b4).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Vegetation Index 700'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Visible Atmospherically Resistant Index Green'] = (b3 - b4) / (b3 + b4 + b2)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Visible Atmospherically Resistant Indices RedEdge'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Wide Dynamic Range Vegetation Index'] = (0.1 * b8 - b4) / (0.1 * b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ashburn Vegetation Index'] = 2 * b9 - b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Canopy Chlorophyll Content Index'] = ((b8 - b5) / (b8 + b5)) / ((b8 - b4) / (b8 + b4))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:229: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Differenced Vegetation Index MSS'] = 2.4 * b9 - b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric iron, Fe2+'] = b12 / b8 + b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric iron, Fe3+'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric Oxides'] = b11 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferrous iron'] = b12 / b8 + b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:234: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Global Vegetation Moisture Index'] = ((b8 + 0.1) - (b12 + 0.02)) / ((b8 + 0.1) + (b12 + 0.02))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:235: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mid-infrared vegetation index'] = b9 / b11\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:236: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Green Vegetation Index'] = -0.386 * b3 - 0.530 * b4 + 0.535 * b6 + 0.532 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:237: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Non Such Index']         =  0.404 * b3 - 0.039 * b4 - 0.505 * b6 + 0.762 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Soil Brightness Index']  =  0.406 * b3 + 0.600 * b4 + 0.645 * b6 + 0.243 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Yellow Vegetation Index'] = 0.723 * b3 - 0.597 * b4 + 0.206 * b6 - 0.278 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Nonlinear vegetation index'] = (b9 ** 2 - b4) / (b9 ** 2 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference MIR/NIR'] = (b12 - b8) / (b12 + b8)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/MIR'] = (b8 - b12) / (b8 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/SWIR'] = (b8 - b12) / (b8 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:244: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['SAVImir'] = (b8 - b12) * (1 + 0.48) / (b8 + b12 + 0.48)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio MIR/NIR'] = b12 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio MIR/Red'] = b12 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/MIR'] = b8 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:248: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Background Line'] = b9 -2.4 * b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:249: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Composition Index'] = (b11 - b8) / (b11 + b8)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Specific Leaf Area Vegetation Index'] = b8 / (b4 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Green Vegetation Index MSS'] = -0.283 * b3 - 0.660 * b4 + 0.577 * b6 + 0.388 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Non Such Index MSS']         = -0.016 * b3 + 0.131 * b4 - 0.425 * b6 + 0.882 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Soil Brightness Index MSS']  =  0.332 * b3 + 0.603 * b4 + 0.675 * b6 + 0.262 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:255: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - vegetation']                 = -0.2848 * b2 - 0.2435 * b3 - 0.5436 * b4 + 0.7243 * b8 + 0.0840 * b11 - 0.1800 * b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - wetness']                    = 0.1509 * b2 + 0.1973 * b3 + 0.3279 * b4 + 0.3406 * b8 - 0.7112 * b11 - 0.4572 * b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Yellow Vegetation Index MSS'] = -0.899 * b3 + 0.428 * b4 + 0.076 * b6 - 0.041 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red edge 1'] = b5 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red edge 2'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Blue NDVI'] = (b8 - (b4 + b2)) / (b8 + (b4 + b2))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 1'] = 700 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 2'] = 702 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Inflection Point 3'] = 705 + 35 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Red-Edge Position Linear Interpolation'] = 700 + 40 * ((((b4 + b7) / 2) - b5) / (b6 - b5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Reflectance at the inflexion point'] = (b4 + b7) / 2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Renormalized Difference Vegetation Index'] = (b8 - b4) / ((b8 + b4).pow(0.5))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:160: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Shape Index'] = (2 * b4 - b3 - b2) / (b3 - b2)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 1600/820'] = b11 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 1650/2218'] = b11 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 440/740'] = b1 / b6\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 450/550'] = b1 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:165: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 520/670'] = b2 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:166: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 550/800'] = b3 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 560/658'] = b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 675/555'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 675/705'] = b4 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 700'] = 1 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:171: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 710/670'] = b5 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 735/710'] = b6 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 774/677'] = b7 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 800/2170'] = b8 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 800/500'] = b8 / b2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:176: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 810/560'] = b8 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 833/1649'] = b8 / b11\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 833/658'] = b8 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 850/710'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 860/550'] = b8a / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio 860/708'] = b8a / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/700-715'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/G'] = b8 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/RED'] = b8 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/Rededge'] = b8 / b5\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:189: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/Blue'] = b4 / b2\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/Green'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio Red/NIR'] = b4 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Adjusted Vegetation Index'] = (b8 - b4) / (b8 + b4 + 0.48) * (1 + 0.48)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['SQRT(IR/R)'] = (b8 / b4).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:203: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Structure Intensive Pigment Index 1'] = (b8 - b1) / (b8 - b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:204: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Structure Intensive Pigment Index 3'] = (b8 - b2) / (b8 - b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:212: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['TCARI/OSAVI'] = (3 * ((b5 - b4) - (0.2 * (b5 - b3) * (b5 / b4)))) / ((1 + 0.16) * ((b8 - b4)/(b8 + b4 + 0.16)))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:213: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Transformed Chlorophyll Absorbtion Ratio'] = 3 * ((b5 - b4) - (0.2 * (b5 - b3) * (b5 / b4)))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Transformed NDVI'] = ((b8 - b4) / (b8 + b4) + 0.5).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:218: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Triangular chlorophyll index'] = 1.2 * (b5 - b3) - 1.5 * (b4 - b3) * (b5 / b4).pow(0.5)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:219: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Vegetation Index 700'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Visible Atmospherically Resistant Index Green'] = (b3 - b4) / (b3 + b4 + b2)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Visible Atmospherically Resistant Indices RedEdge'] = (b5 - b4) / (b5 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:224: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Wide Dynamic Range Vegetation Index'] = (0.1 * b8 - b4) / (0.1 * b8 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ashburn Vegetation Index'] = 2 * b9 - b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Canopy Chlorophyll Content Index'] = ((b8 - b5) / (b8 + b5)) / ((b8 - b4) / (b8 + b4))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:229: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Differenced Vegetation Index MSS'] = 2.4 * b9 - b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:230: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric iron, Fe2+'] = b12 / b8 + b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:231: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric iron, Fe3+'] = b4 / b3\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferric Oxides'] = b11 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:233: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Ferrous iron'] = b12 / b8 + b3 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:234: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Global Vegetation Moisture Index'] = ((b8 + 0.1) - (b12 + 0.02)) / ((b8 + 0.1) + (b12 + 0.02))\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:235: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Mid-infrared vegetation index'] = b9 / b11\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:236: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Green Vegetation Index'] = -0.386 * b3 - 0.530 * b4 + 0.535 * b6 + 0.532 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:237: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Non Such Index']         =  0.404 * b3 - 0.039 * b4 - 0.505 * b6 + 0.762 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:238: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Soil Brightness Index']  =  0.406 * b3 + 0.600 * b4 + 0.645 * b6 + 0.243 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Misra Yellow Vegetation Index'] = 0.723 * b3 - 0.597 * b4 + 0.206 * b6 - 0.278 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:240: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Nonlinear vegetation index'] = (b9 ** 2 - b4) / (b9 ** 2 + b4)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:241: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference MIR/NIR'] = (b12 - b8) / (b12 + b8)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:242: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/MIR'] = (b8 - b12) / (b8 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Normalized Difference NIR/SWIR'] = (b8 - b12) / (b8 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:244: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['SAVImir'] = (b8 - b12) * (1 + 0.48) / (b8 + b12 + 0.48)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio MIR/NIR'] = b12 / b8\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:246: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio MIR/Red'] = b12 / b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Simple Ratio NIR/MIR'] = b8 / b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:248: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Background Line'] = b9 -2.4 * b4\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:249: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Soil Composition Index'] = (b11 - b8) / (b11 + b8)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:250: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Specific Leaf Area Vegetation Index'] = b8 / (b4 + b12)\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:252: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Green Vegetation Index MSS'] = -0.283 * b3 - 0.660 * b4 + 0.577 * b6 + 0.388 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:253: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Non Such Index MSS']         = -0.016 * b3 + 0.131 * b4 - 0.425 * b6 + 0.882 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:254: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Soil Brightness Index MSS']  =  0.332 * b3 + 0.603 * b4 + 0.675 * b6 + 0.262 * b9\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:255: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - vegetation']                 = -0.2848 * b2 - 0.2435 * b3 - 0.5436 * b4 + 0.7243 * b8 + 0.0840 * b11 - 0.1800 * b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - wetness']                    = 0.1509 * b2 + 0.1973 * b3 + 0.3279 * b4 + 0.3406 * b8 - 0.7112 * b11 - 0.4572 * b12\n",
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_32292\\269966182.py:257: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Tasselled Cap - Yellow Vegetation Index MSS'] = -0.899 * b3 + 0.428 * b4 + 0.076 * b6 - 0.041 * b9\n"
     ]
    }
   ],
   "source": [
    "DEF_NONDEF_df = DEF_NONDEF_df.apply(add_features_for_DEF_NONDEF , axis = 1)\n",
    "test_df = test_df.apply(add_features_for_DEF_NONDEF , axis = 1)\n",
    "\n",
    "DEF_NONDEF_df = drop_features_for_DEF_NONDEF(DEF_NONDEF_df)\n",
    "test_df = drop_features_for_DEF_NONDEF(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "NON_DEF    10468\n",
       "DEF         2585\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEF_NONDEF_df['nforest_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN , SMOTETomek\n\u001b[0;32m      4\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m , sampling_strategy\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m DEF_NONDEF_df , label_df  \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(DEF_NONDEF_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnforest_type\u001b[39m\u001b[38;5;124m'\u001b[39m]) , DEF_NONDEF_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnforest_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m DEF_NONDEF_df \u001b[38;5;241m=\u001b[39m DEF_NONDEF_df\u001b[38;5;241m.\u001b[39mjoin(label_df)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\anaconda\\Lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1187\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     )\n\u001b[1;32m-> 1192\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1193\u001b[0m     X,\n\u001b[0;32m   1194\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1195\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1196\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1197\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1198\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1199\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1200\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1201\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1202\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1203\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1204\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1205\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1206\u001b[0m )\n\u001b[0;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1000\u001b[0m     )\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1003\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1004\u001b[0m         array,\n\u001b[0;32m   1005\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1006\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1007\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    127\u001b[0m     X,\n\u001b[0;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    133\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE , KMeansSMOTE\n",
    "from imblearn.combine import SMOTEENN , SMOTETomek\n",
    "\n",
    "smote = SMOTE(random_state = 42 , sampling_strategy= 'all')\n",
    "\n",
    "DEF_NONDEF_df , label_df  = smote.fit_resample(DEF_NONDEF_df.drop(columns=['nforest_type']) , DEF_NONDEF_df['nforest_type'])\n",
    "DEF_NONDEF_df = DEF_NONDEF_df.join(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "NON_DEF    10468\n",
       "DEF        10468\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEF_NONDEF_df['nforest_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240605_090725\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240605_090725\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       2.07 GB / 15.28 GB (13.6%)\n",
      "Disk Space Avail:   594.54 GB / 952.03 GB (62.4%)\n",
      "===================================================\n",
      "Train Data Rows:    20936\n",
      "Train Data Columns: 26\n",
      "Label Column:       nforest_type\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  ['NON_DEF', 'DEF']\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = NON_DEF, class 0 = DEF\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (NON_DEF) vs negative (DEF) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2108.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.15 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['NDVI', 'EVI', 'NDWI ', 'SAVI ', 'MSAVI', ...]\n",
      "\t\t('int', [])   : 12 | ['b1', 'b11', 'b12', 'b2', 'b3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['NDVI', 'EVI', 'NDWI ', 'SAVI ', 'MSAVI', ...]\n",
      "\t\t('int', [])   : 12 | ['b1', 'b11', 'b12', 'b2', 'b3', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t26 features in original data used to generate 26 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.15 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 18842, Val Rows: 2094\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.8988\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9083\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0864374\n",
      "[2000]\tvalid_set's binary_error: 0.0687679\n",
      "[3000]\tvalid_set's binary_error: 0.0592168\n",
      "[4000]\tvalid_set's binary_error: 0.0573066\n",
      "[5000]\tvalid_set's binary_error: 0.056829\n",
      "[6000]\tvalid_set's binary_error: 0.0563515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9465\t = Validation score   (accuracy)\n",
      "\t5.37s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0673352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9389\t = Validation score   (accuracy)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9207\t = Validation score   (accuracy)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9226\t = Validation score   (accuracy)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9451\t = Validation score   (accuracy)\n",
      "\t42.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9236\t = Validation score   (accuracy)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9193\t = Validation score   (accuracy)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9136\t = Validation score   (accuracy)\n",
      "\t12.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9394\t = Validation score   (accuracy)\n",
      "\t2.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9494\t = Validation score   (accuracy)\n",
      "\t60.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0582617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9446\t = Validation score   (accuracy)\n",
      "\t4.66s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.333, 'KNeighborsDist': 0.133, 'RandomForestGini': 0.133, 'CatBoost': 0.133, 'ExtraTreesGini': 0.133, 'LightGBMXT': 0.067, 'RandomForestEntr': 0.067}\n",
      "\t0.9599\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 139.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240605_090725\")\n"
     ]
    }
   ],
   "source": [
    "label = 'nforest_type'\n",
    "predictor = TabularPredictor(label = label).fit(DEF_NONDEF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_FULL ...\n",
      "\t0.03s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_FULL ...\n",
      "\t3.85s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_FULL ...\n",
      "\t1.29s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestGini_FULL ...\n",
      "\t2.57s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestEntr_FULL ...\n",
      "\t3.62s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_FULL ...\n",
      "\t23.78s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesGini_FULL ...\n",
      "\t0.94s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesEntr_FULL ...\n",
      "\t1.0s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_FULL ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t10.09s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_FULL ...\n",
      "\t1.61s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_FULL ...\n",
      "\t55.98s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_FULL ...\n",
      "\t3.39s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.333, 'KNeighborsDist': 0.133, 'RandomForestGini': 0.133, 'CatBoost': 0.133, 'ExtraTreesGini': 0.133, 'LightGBMXT': 0.067, 'RandomForestEntr': 0.067}\n",
      "\t0.09s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 111.16s ... Best model: \"WeightedEnsemble_L2_FULL\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNeighborsUnif': 'KNeighborsUnif_FULL',\n",
       " 'KNeighborsDist': 'KNeighborsDist_FULL',\n",
       " 'LightGBMXT': 'LightGBMXT_FULL',\n",
       " 'LightGBM': 'LightGBM_FULL',\n",
       " 'RandomForestGini': 'RandomForestGini_FULL',\n",
       " 'RandomForestEntr': 'RandomForestEntr_FULL',\n",
       " 'CatBoost': 'CatBoost_FULL',\n",
       " 'ExtraTreesGini': 'ExtraTreesGini_FULL',\n",
       " 'ExtraTreesEntr': 'ExtraTreesEntr_FULL',\n",
       " 'NeuralNetFastAI': 'NeuralNetFastAI_FULL',\n",
       " 'XGBoost': 'XGBoost_FULL',\n",
       " 'NeuralNetTorch': 'NeuralNetTorch_FULL',\n",
       " 'LightGBMLarge': 'LightGBMLarge_FULL',\n",
       " 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Given test_data for pseudo labeling did not contain labels. AutoGluon will assign pseudo labels to data and use it for extra training data...\n",
      "Beginning iteration 1 of pseudolabeling out of max 3\n",
      "Pseudolabeling algorithm confidently assigned pseudolabels to 442 rows of data on iteration 1. Adding to train data\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_PSEUDO_1 ...\n",
      "\t0.8983\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_PSEUDO_1 ...\n",
      "\t0.9078\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_PSEUDO_1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0830946\n",
      "[2000]\tvalid_set's binary_error: 0.0668577\n",
      "[3000]\tvalid_set's binary_error: 0.0635148\n",
      "[4000]\tvalid_set's binary_error: 0.0596944\n",
      "[5000]\tvalid_set's binary_error: 0.0558739\n",
      "[6000]\tvalid_set's binary_error: 0.0573066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9451\t = Validation score   (accuracy)\n",
      "\t5.34s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_PSEUDO_1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.0639924\n",
      "[2000]\tvalid_set's binary_error: 0.0530086\n",
      "[3000]\tvalid_set's binary_error: 0.052531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9499\t = Validation score   (accuracy)\n",
      "\t3.15s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_PSEUDO_1 ...\n",
      "\t0.9202\t = Validation score   (accuracy)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_PSEUDO_1 ...\n",
      "\t0.9222\t = Validation score   (accuracy)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_PSEUDO_1 ...\n",
      "\t0.9475\t = Validation score   (accuracy)\n",
      "\t72.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_PSEUDO_1 ...\n",
      "\t0.916\t = Validation score   (accuracy)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_PSEUDO_1 ...\n",
      "\t0.9183\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_PSEUDO_1 ...\n",
      "\t0.9155\t = Validation score   (accuracy)\n",
      "\t13.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_PSEUDO_1 ...\n",
      "\t0.9417\t = Validation score   (accuracy)\n",
      "\t2.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_PSEUDO_1 ...\n",
      "\t0.9546\t = Validation score   (accuracy)\n",
      "\t89.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_PSEUDO_1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.056829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.9441\t = Validation score   (accuracy)\n",
      "\t4.51s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2_PSEUDO_1 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch_PSEUDO_1': 0.35, 'KNeighborsDist_PSEUDO_1': 0.25, 'ExtraTreesEntr_PSEUDO_1': 0.1, 'XGBoost_PSEUDO_1': 0.1, 'LightGBM_PSEUDO_1': 0.05, 'ExtraTreesGini_PSEUDO_1': 0.05, 'NeuralNetFastAI_PSEUDO_1': 0.05, 'LightGBMLarge_PSEUDO_1': 0.05}\n",
      "\t0.9618\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240605_090725\")\n",
      "Pseudolabeling algorithm changed validation score from: 0.9598853868194842, to: 0.9598853868194842 using evaluation metric: accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x164a030d0d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_pseudolabel(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 26 features using 5000 rows with 5 shuffle sets...\n",
      "\t154.81s\t= Expected runtime (30.96s per shuffle set)\n",
      "\t47.88s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b11</th>\n",
       "      <td>0.24492</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>1.547276e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250294</td>\n",
       "      <td>0.239546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8_a</th>\n",
       "      <td>0.16952</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>1.691402e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.173323</td>\n",
       "      <td>0.165717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b9</th>\n",
       "      <td>0.11240</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>5.869694e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.118522</td>\n",
       "      <td>0.106278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b6</th>\n",
       "      <td>0.10120</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>6.602240e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b12</th>\n",
       "      <td>0.09140</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>1.525841e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.102651</td>\n",
       "      <td>0.080149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>0.08508</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>1.364722e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.086889</td>\n",
       "      <td>0.083271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b7</th>\n",
       "      <td>0.07880</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>7.999655e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.087051</td>\n",
       "      <td>0.070549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1</th>\n",
       "      <td>0.07224</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>1.033886e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.080307</td>\n",
       "      <td>0.064173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRVI</th>\n",
       "      <td>0.06680</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>1.340748e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071273</td>\n",
       "      <td>0.062327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8</th>\n",
       "      <td>0.06344</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>2.131042e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071932</td>\n",
       "      <td>0.054948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBR</th>\n",
       "      <td>0.06192</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>3.143278e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.067052</td>\n",
       "      <td>0.056788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2</th>\n",
       "      <td>0.05204</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>1.263880e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.058151</td>\n",
       "      <td>0.045929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCARI</th>\n",
       "      <td>0.05024</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>1.029644e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.055844</td>\n",
       "      <td>0.044636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RENDVI</th>\n",
       "      <td>0.04756</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>1.610953e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053495</td>\n",
       "      <td>0.041625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b3</th>\n",
       "      <td>0.02520</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>7.071578e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.029758</td>\n",
       "      <td>0.020642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDMI</th>\n",
       "      <td>0.02308</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>9.437192e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027568</td>\n",
       "      <td>0.018592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b4</th>\n",
       "      <td>0.01856</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>3.927559e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.023731</td>\n",
       "      <td>0.013389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSI</th>\n",
       "      <td>0.01852</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>5.769230e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024208</td>\n",
       "      <td>0.012832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSI</th>\n",
       "      <td>0.01472</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>3.975002e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018834</td>\n",
       "      <td>0.010606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVI</th>\n",
       "      <td>0.01408</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>2.203765e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>0.012179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSAVI</th>\n",
       "      <td>0.00568</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>6.321869e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.003894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNDVI</th>\n",
       "      <td>0.00512</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>2.983767e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.002726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDWI</th>\n",
       "      <td>0.00500</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>3.994984e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.002478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVI</th>\n",
       "      <td>0.00416</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>9.947768e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.002692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDVI</th>\n",
       "      <td>0.00356</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>7.915787e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVI</th>\n",
       "      <td>0.00296</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>4.028728e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004456</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance    stddev       p_value  n  p99_high   p99_low\n",
       "b11         0.24492  0.002610  1.547276e-09  5  0.250294  0.239546\n",
       "b8_a        0.16952  0.001847  1.691402e-09  5  0.173323  0.165717\n",
       "b9          0.11240  0.002973  5.869694e-08  5  0.118522  0.106278\n",
       "b6          0.10120  0.004905  6.602240e-07  5  0.111300  0.091100\n",
       "b12         0.09140  0.005464  1.525841e-06  5  0.102651  0.080149\n",
       "b5          0.08508  0.000879  1.364722e-09  5  0.086889  0.083271\n",
       "b7          0.07880  0.004007  7.999655e-07  5  0.087051  0.070549\n",
       "b1          0.07224  0.003918  1.033886e-06  5  0.080307  0.064173\n",
       "GRVI        0.06680  0.002173  1.340748e-07  5  0.071273  0.062327\n",
       "b8          0.06344  0.004124  2.131042e-06  5  0.071932  0.054948\n",
       "NBR         0.06192  0.002492  3.143278e-07  5  0.067052  0.056788\n",
       "b2          0.05204  0.002968  1.263880e-06  5  0.058151  0.045929\n",
       "MCARI       0.05024  0.002722  1.029644e-06  5  0.055844  0.044636\n",
       "RENDVI      0.04756  0.002882  1.610953e-06  5  0.053495  0.041625\n",
       "b3          0.02520  0.002214  7.071578e-06  5  0.029758  0.020642\n",
       "NDMI        0.02308  0.002180  9.437192e-06  5  0.027568  0.018592\n",
       "b4          0.01856  0.002512  3.927559e-05  5  0.023731  0.013389\n",
       "MSI         0.01852  0.002763  5.769230e-05  5  0.024208  0.012832\n",
       "BSI         0.01472  0.001998  3.975002e-05  5  0.018834  0.010606\n",
       "EVI         0.01408  0.000923  2.203765e-06  5  0.015981  0.012179\n",
       "MSAVI       0.00568  0.000867  6.321869e-05  5  0.007466  0.003894\n",
       "GNDVI       0.00512  0.001163  2.983767e-04  5  0.007514  0.002726\n",
       "NDWI        0.00500  0.001225  3.994984e-04  5  0.007522  0.002478\n",
       "TVI         0.00416  0.000713  9.947768e-05  5  0.005628  0.002692\n",
       "NDVI        0.00356  0.001043  7.915787e-04  5  0.005708  0.001412\n",
       "SAVI        0.00296  0.000727  4.028728e-04  5  0.004456  0.001464"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = predictor.feature_importance(DEF_NONDEF_df)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.to_csv('./features/DEF_NONDEF_features_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "NON_DEF    3011\n",
       "DEF         989\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1        NON_DEF\n",
       "2        NON_DEF\n",
       "5        NON_DEF\n",
       "7        NON_DEF\n",
       "12           DEF\n",
       "          ...   \n",
       "17039    NON_DEF\n",
       "17042    NON_DEF\n",
       "17043    NON_DEF\n",
       "17047    NON_DEF\n",
       "17052    NON_DEF\n",
       "Name: nforest_type, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "12       DEF\n",
       "19       DEF\n",
       "29       DEF\n",
       "50       DEF\n",
       "51       DEF\n",
       "        ... \n",
       "17013    DEF\n",
       "17024    DEF\n",
       "17026    DEF\n",
       "17032    DEF\n",
       "17034    DEF\n",
       "Name: nforest_type, Length: 989, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_DEF_ONLY = prediction[prediction == 'DEF']\n",
    "prediction_DEF_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDF_DDF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./datasets/test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teehe\\AppData\\Local\\Temp\\ipykernel_24200\\254591184.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  test_df = test_df[prediction == 'NON_DEF']\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df[prediction == 'NON_DEF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_df = pd.read_csv('./datasets/MDF_DDF.csv' , index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_for_MDF_DDF(row) :\n",
    "\n",
    "    row['NDVI'] = (row['b8'] - row['b4']) / (row['b8'] + row['b4'])\n",
    "    row['EVI'] = 2.5 * ((row['b8'] - row['b4']) / (row['b8'] + 6 * row['b4'] - 7.5 * row['b2'] + 1.01))\n",
    "    row['NDWI '] = (row['b3'] - row['b8']) / (row['b3'] + row['b8'])\n",
    "    row['SAVI '] = (row['b8'] - row['b4']) * (1 + 0.5) / (row['b8'] + row['b4'] + 0.5)\n",
    "    row['MSAVI'] = (2 * row['b8'] + 1 - ( (2 * row['b8'] + 1) ** 2 - 8 * (row['b8'] - row['b4'])) ** (1 / 2)) / 2\n",
    "    row['GNDVI '] = (row['b8'] - row['b3']) / (row['b8'] + row['b3'])\n",
    "    row['RENDVI '] = (row['b8'] - row['b5']) / (row['b8'] + row['b5'])\n",
    "    row['NDMI '] = (row['b8'] - row['b11']) / (row['b8'] + row['b11'])\n",
    "    row['GRVI'] = (row['b3'] - row['b4']) / (row['b3'] + row['b4'])\n",
    "    row['TVI'] = ( (row['b8'] - row['b4']) / (row['b8'] + row['b4'] + 0.5) ) ** (1 / 2)\n",
    "    row['MCARI'] = ((row['b5'] - row['b4']) - 0.2 * (row['b5'] - row['b3'])) / (row['b5'] / row['b4'])\n",
    "    row['BSI'] =  ((row['b11'] + row['b4']) - (row['b8'] + row['b2'])) / ((row['b11'] + row['b4']) + (row['b8'] + row['b2']))\n",
    "    row['NBR'] = (row['b8'] - row['b12']) / (row['b8'] + row['b12'])\n",
    "    row['MSI'] = row['b11'] / row['b8']\n",
    "\n",
    "    return row\n",
    "\n",
    "def drop_features_for_MDF_DDF(df) :\n",
    "\n",
    "    return df\n",
    "    # return df.drop(columns = ['b1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDF_DDF_df = MDF_DDF_df.apply(add_features_for_MDF_DDF , axis = 1)\n",
    "MDF_DDF_df = drop_features_for_MDF_DDF(MDF_DDF_df)\n",
    "\n",
    "test_df = test_df.apply(add_features_for_MDF_DDF , axis = 1)\n",
    "test_df = drop_features_for_MDF_DDF(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "MDF    5865\n",
       "DDF    4603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDF_DDF_df['nforest_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE , KMeansSMOTE\n",
    "from imblearn.combine import SMOTEENN , SMOTETomek\n",
    "\n",
    "smote = SMOTE(random_state = 42 , sampling_strategy= 'all')\n",
    "\n",
    "MDF_DDF_df , label_df  = smote.fit_resample(MDF_DDF_df.drop(columns=['nforest_type']) , MDF_DDF_df['nforest_type'])\n",
    "MDF_DDF_df = MDF_DDF_df.join(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "MDF    5865\n",
       "DDF    5865\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDF_DDF_df['nforest_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240605_091623\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240605_091623\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.7\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       2.50 GB / 15.28 GB (16.3%)\n",
      "Disk Space Avail:   592.68 GB / 952.03 GB (62.3%)\n",
      "===================================================\n",
      "Train Data Rows:    11730\n",
      "Train Data Columns: 26\n",
      "Label Column:       nforest_type\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  ['MDF', 'DDF']\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = MDF, class 0 = DDF\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (MDF) vs negative (DDF) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2555.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.33 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['NDVI', 'EVI', 'NDWI ', 'SAVI ', 'MSAVI', ...]\n",
      "\t\t('int', [])   : 12 | ['b1', 'b11', 'b12', 'b2', 'b3', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 14 | ['NDVI', 'EVI', 'NDWI ', 'SAVI ', 'MSAVI', ...]\n",
      "\t\t('int', [])   : 12 | ['b1', 'b11', 'b12', 'b2', 'b3', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t26 features in original data used to generate 26 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10557, Val Rows: 1173\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.7272\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.7425\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.244672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7621\t = Validation score   (accuracy)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.7681\t = Validation score   (accuracy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.236999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7613\t = Validation score   (accuracy)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7579\t = Validation score   (accuracy)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.7442\t = Validation score   (accuracy)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7656\t = Validation score   (accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7621\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7596\t = Validation score   (accuracy)\n",
      "\t6.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7613\t = Validation score   (accuracy)\n",
      "\t2.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7749\t = Validation score   (accuracy)\n",
      "\t23.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.7749\t = Validation score   (accuracy)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'KNeighborsDist': 0.286, 'NeuralNetFastAI': 0.238, 'NeuralNetTorch': 0.238, 'RandomForestGini': 0.19, 'KNeighborsUnif': 0.048}\n",
      "\t0.798\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240605_091623\")\n"
     ]
    }
   ],
   "source": [
    "predictor_for_MDF_DDF = TabularPredictor(label = label).fit(MDF_DDF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsUnif_FULL ...\n",
      "\t0.02s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: KNeighborsDist_FULL ...\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_FULL ...\n",
      "\t1.1s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_FULL ...\n",
      "\t0.87s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestGini_FULL ...\n",
      "\t1.63s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: RandomForestEntr_FULL ...\n",
      "\t2.4s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_FULL ...\n",
      "\t3.14s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesGini_FULL ...\n",
      "\t0.84s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: ExtraTreesEntr_FULL ...\n",
      "\t0.88s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_FULL ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t4.82s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_FULL ...\n",
      "\t1.88s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_FULL ...\n",
      "\t20.39s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_FULL ...\n",
      "\t1.33s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'KNeighborsDist': 0.286, 'NeuralNetFastAI': 0.238, 'NeuralNetTorch': 0.238, 'RandomForestGini': 0.19, 'KNeighborsUnif': 0.048}\n",
      "\t0.1s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 41.34s ... Best model: \"WeightedEnsemble_L2_FULL\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'KNeighborsUnif': 'KNeighborsUnif_FULL',\n",
       " 'KNeighborsDist': 'KNeighborsDist_FULL',\n",
       " 'LightGBMXT': 'LightGBMXT_FULL',\n",
       " 'LightGBM': 'LightGBM_FULL',\n",
       " 'RandomForestGini': 'RandomForestGini_FULL',\n",
       " 'RandomForestEntr': 'RandomForestEntr_FULL',\n",
       " 'CatBoost': 'CatBoost_FULL',\n",
       " 'ExtraTreesGini': 'ExtraTreesGini_FULL',\n",
       " 'ExtraTreesEntr': 'ExtraTreesEntr_FULL',\n",
       " 'NeuralNetFastAI': 'NeuralNetFastAI_FULL',\n",
       " 'XGBoost': 'XGBoost_FULL',\n",
       " 'NeuralNetTorch': 'NeuralNetTorch_FULL',\n",
       " 'LightGBMLarge': 'LightGBMLarge_FULL',\n",
       " 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_for_MDF_DDF.refit_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Given test_data for pseudo labeling did not contain labels. AutoGluon will assign pseudo labels to data and use it for extra training data...\n",
      "Beginning iteration 1 of pseudolabeling out of max 3\n",
      "Pseudolabeling algorithm confidently assigned pseudolabels to 98 rows of data on iteration 1. Adding to train data\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_PSEUDO_1 ...\n",
      "\t0.7272\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_PSEUDO_1 ...\n",
      "\t0.7425\t = Validation score   (accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_PSEUDO_1 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.238704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7681\t = Validation score   (accuracy)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_PSEUDO_1 ...\n",
      "\t0.7724\t = Validation score   (accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_PSEUDO_1 ...\n",
      "\t0.763\t = Validation score   (accuracy)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_PSEUDO_1 ...\n",
      "\t0.7579\t = Validation score   (accuracy)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_PSEUDO_1 ...\n",
      "\t0.7732\t = Validation score   (accuracy)\n",
      "\t29.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_PSEUDO_1 ...\n",
      "\t0.7639\t = Validation score   (accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_PSEUDO_1 ...\n",
      "\t0.7613\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_PSEUDO_1 ...\n",
      "\t0.7613\t = Validation score   (accuracy)\n",
      "\t6.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost_PSEUDO_1 ...\n",
      "\t0.7621\t = Validation score   (accuracy)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_PSEUDO_1 ...\n",
      "\t0.7758\t = Validation score   (accuracy)\n",
      "\t20.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_PSEUDO_1 ...\n",
      "\t0.7792\t = Validation score   (accuracy)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2_PSEUDO_1 ...\n",
      "\tEnsemble Weights: {'KNeighborsDist_PSEUDO_1': 0.333, 'NeuralNetTorch_PSEUDO_1': 0.333, 'LightGBMLarge_PSEUDO_1': 0.333}\n",
      "\t0.7971\t = Validation score   (accuracy)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240605_091623\")\n",
      "Pseudolabeling algorithm changed validation score from: 0.7979539641943734, to: 0.7979539641943734 using evaluation metric: accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x164b6fe0110>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_for_MDF_DDF.fit_pseudolabel(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_MDF_DDF = predictor_for_MDF_DDF.predict(add_features_for_MDF_DDF( test_df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_MDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prediction_MDF\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_MDF' is not defined"
     ]
    }
   ],
   "source": [
    "prediction_MDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 26 features using 5000 rows with 5 shuffle sets...\n",
      "\t119.86s\t= Expected runtime (23.97s per shuffle set)\n",
      "\t40.46s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b9</th>\n",
       "      <td>0.28252</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>4.096252e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.296584</td>\n",
       "      <td>0.268456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b7</th>\n",
       "      <td>0.22984</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>1.005597e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.244163</td>\n",
       "      <td>0.215517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8</th>\n",
       "      <td>0.19548</td>\n",
       "      <td>0.007180</td>\n",
       "      <td>2.180129e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.210264</td>\n",
       "      <td>0.180696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b12</th>\n",
       "      <td>0.19240</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>4.296887e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.202093</td>\n",
       "      <td>0.182707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b11</th>\n",
       "      <td>0.18432</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>9.566407e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.195664</td>\n",
       "      <td>0.172976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b5</th>\n",
       "      <td>0.18248</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>1.613175e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.189675</td>\n",
       "      <td>0.175285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8_a</th>\n",
       "      <td>0.18012</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>2.485013e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.194196</td>\n",
       "      <td>0.166044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b6</th>\n",
       "      <td>0.16544</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>1.057334e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.175880</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1</th>\n",
       "      <td>0.13084</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>1.118699e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.135548</td>\n",
       "      <td>0.126132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b3</th>\n",
       "      <td>0.11756</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>1.919343e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.120282</td>\n",
       "      <td>0.114838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2</th>\n",
       "      <td>0.11196</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>5.165808e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.117866</td>\n",
       "      <td>0.106054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b4</th>\n",
       "      <td>0.10796</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>9.779094e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.114641</td>\n",
       "      <td>0.101279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCARI</th>\n",
       "      <td>0.06888</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>2.331722e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.071858</td>\n",
       "      <td>0.065902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRVI</th>\n",
       "      <td>0.03924</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>5.227781e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>0.032661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NBR</th>\n",
       "      <td>0.03860</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>3.857760e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.044596</td>\n",
       "      <td>0.032604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RENDVI</th>\n",
       "      <td>0.02192</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>7.789127e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.017858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNDVI</th>\n",
       "      <td>0.01928</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>9.942019e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.021412</td>\n",
       "      <td>0.017148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDWI</th>\n",
       "      <td>0.01652</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>7.798884e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019582</td>\n",
       "      <td>0.013458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSI</th>\n",
       "      <td>0.01168</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>1.267634e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.007297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDMI</th>\n",
       "      <td>0.01156</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>1.217081e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015853</td>\n",
       "      <td>0.007267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EVI</th>\n",
       "      <td>0.00940</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>3.229810e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>0.008004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BSI</th>\n",
       "      <td>0.00752</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>5.014856e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.005291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVI</th>\n",
       "      <td>0.00736</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>1.903390e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009068</td>\n",
       "      <td>0.005652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDVI</th>\n",
       "      <td>0.00648</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>3.673540e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009678</td>\n",
       "      <td>0.003282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVI</th>\n",
       "      <td>0.00560</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>6.774432e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.002357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSAVI</th>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>3.606338e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         importance    stddev       p_value  n  p99_high   p99_low\n",
       "b9          0.28252  0.006830  4.096252e-08  5  0.296584  0.268456\n",
       "b7          0.22984  0.006956  1.005597e-07  5  0.244163  0.215517\n",
       "b8          0.19548  0.007180  2.180129e-07  5  0.210264  0.180696\n",
       "b12         0.19240  0.004707  4.296887e-08  5  0.202093  0.182707\n",
       "b11         0.18432  0.005509  9.566407e-08  5  0.195664  0.172976\n",
       "b5          0.18248  0.003495  1.613175e-08  5  0.189675  0.175285\n",
       "b8_a        0.18012  0.006836  2.485013e-07  5  0.194196  0.166044\n",
       "b6          0.16544  0.005070  1.057334e-07  5  0.175880  0.155000\n",
       "b1          0.13084  0.002286  1.118699e-08  5  0.135548  0.126132\n",
       "b3          0.11756  0.001322  1.919343e-09  5  0.120282  0.114838\n",
       "b2          0.11196  0.002868  5.165808e-08  5  0.117866  0.106054\n",
       "b4          0.10796  0.003245  9.779094e-08  5  0.114641  0.101279\n",
       "MCARI       0.06888  0.001446  2.331722e-08  5  0.071858  0.065902\n",
       "GRVI        0.03924  0.003195  5.227781e-06  5  0.045819  0.032661\n",
       "NBR         0.03860  0.002912  3.857760e-06  5  0.044596  0.032604\n",
       "RENDVI      0.02192  0.001973  7.789127e-06  5  0.025982  0.017858\n",
       "GNDVI       0.01928  0.001035  9.942019e-07  5  0.021412  0.017148\n",
       "NDWI        0.01652  0.001487  7.798884e-06  5  0.019582  0.013458\n",
       "MSI         0.01168  0.002129  1.267634e-04  5  0.016063  0.007297\n",
       "NDMI        0.01156  0.002085  1.217081e-04  5  0.015853  0.007267\n",
       "EVI         0.00940  0.000678  3.229810e-06  5  0.010796  0.008004\n",
       "BSI         0.00752  0.001083  5.014856e-05  5  0.009749  0.005291\n",
       "SAVI        0.00736  0.000829  1.903390e-05  5  0.009068  0.005652\n",
       "NDVI        0.00648  0.001553  3.673540e-04  5  0.009678  0.003282\n",
       "TVI         0.00560  0.001575  6.774432e-04  5  0.008843  0.002357\n",
       "MSAVI       0.00448  0.000593  3.606338e-05  5  0.005702  0.003258"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = predictor_for_MDF_DDF.feature_importance(MDF_DDF_df)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.to_csv('./features/MDF_DDF_features_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "DDF    1582\n",
       "MDF    1419\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_MDF_DDF.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1        MDF\n",
       "2        DDF\n",
       "5        MDF\n",
       "7        DDF\n",
       "12       DEF\n",
       "        ... \n",
       "17039    DDF\n",
       "17042    DDF\n",
       "17043    DDF\n",
       "17047    MDF\n",
       "17052    DDF\n",
       "Name: nforest_type, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.concat([prediction_DEF_ONLY , prediction_MDF_DDF])\n",
    "submission_df = submission_df.sort_index()\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nforest_type\n",
       "DDF    1582\n",
       "MDF    1419\n",
       "DEF     999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = './submissions'\n",
    "submission_df.to_csv(f'{submission_path}/submission_2over_features_smote_pseudolabeling_refit_double_classifier.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
